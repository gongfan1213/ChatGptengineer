提到，读者可以结合起来学习。
```bash
!openai tools fine_tunes.prepare_data -f dataset/csl_summarize_finetune.jsonl -q
```
输出的日志样例如下所示。
```
Analyzing...
(...) # 省略
Based on the analysis we will perform the following actions:
- [Recommended] Lowercase all your data in column/key `prompt` [Y/n]: Y
- [Recommended] Lowercase all your data in column/key `completion` [Y/n]: Y
- [Recommended] Add a suffix separator ` --> ` to all prompts [Y/n]: Y
- [Recommended] Add a suffix ending `\n` to all completions [Y/n]: Y
- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y
Your data will be written to a new JSONL file. Proceed [Y/n]: Y
Wrote modified file to 'dataset/csl_summarize_finetune_prepared.jsonl'
(...) # 省略
```
当上述脚本执行完之后，在dataset文件夹下，我们会发现一个新生成的文件csl_summarize_finetune_prepared.jsonl，这便是处理好的标准化的数据文件。接下来，创建一个微调任务，指定数据集和模型，OpenAI会自动上传数据集并开始完成这个微调任务。
```python
import openai
import os

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY

!openai api fine_tunes.create \
    -t "./dataset/csl_summarize_finetune_prepared.jsonl" \
    -m ada \
    --no_check_if_files_exist
```
执行以上命令后，输出的日志如下所示。
```
Uploaded file from ./dataset/csl_summarize_finetune_prepared.jsonl: file-gPzuOuiZUDCGO7t0oDYoWQB
Upload progress:  0%|          | 0.00/380k [00:00<?, ?it/s]
Upload progress: 100%|██████████| 380k/380k [00:00<00:00, 239Mit/s]

Created fine-tune: ft-LoKi6mOxIkOtFzcZTrmivKDa
Streaming events until fine-tuning is complete...
(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-05-07 20:27:26] Created fine-tune: ft-LoKi6mOxIkOtFzcZTrmivKDa
[2023-05-07 20:27:45] Fine-tune costs $0.43
[2023-05-07 20:27:45] Fine-tune enqueued. Queue number: 0
[2023-05-07 20:27:46] Fine-tune started
(...)
```
根据上一步的输出，得到微调运行的ID：ft-LoKi6mOxIkOtFzcZTrmivKDa。同时，日志中也会给出预估的微调成本，比如这里是0.43美元。我们可以通过get命令来获取当前执行进度。当能够从日志中找到fine_tuned_model且status（状态）为succeeded时，表明微调任务已经执行成功。
```bash
!openai api fine_tunes.get -i ft-LoKi6mOxIkOtFzcZTrmivKDa
```
微调任务执行成功的日志如下所示。
```json
{
(...)
"fine_tuned_model": "ada:ft-personal-2023-04-15-13-29-50",
(...)
"status": "succeeded",
}
```
还可以通过fine_tunes.results来保存训练过程的记录，从而帮助我们更好地监控模型的运行情况。
```bash
!openai api fine_tunes.results -i ft-LoKi6mOxIkOtFzcZTrmivKDa > dataset/metric.csv
```

微调任务完成后，就可以像使用ChatGPT一样方便地使用自己的微调模型，只需要将模型名称修改为刚才微调好的模型即可，如下所示。
```python
def summarize_text(text, model_name):
    response = openai.Completion.create(
        engine=model_name,
        prompt=f"请对以下文本进行总结，注意总结的凝练性，将总结字数控制在20个字以内：\n{text}",
        temperature=0.3,
        max_tokens=100,
    )

    summarized_text = response.choices[0].text.strip()
    return summarized_text

text = """自动信任协商主要解决跨安全域的信任建立问题，使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂，自动信任协商面临多方面的安全威胁，针对协商的攻击大多超出常规防范措施所保护的范围，因此有必要对自动信任协商中的攻击手段进行专门分析，按攻击特点对自动信任协商中存在的各种攻击方式进行分类，并介绍相应的防御措施，总结当前研究工作的不足，以及对未来的研究进行展望。"""
ada_abs = summarize_text(text, model_name="ada")
ada_ft_abs = summarize_text(text, model_name="ada:ft-personal-2023-04-15-13-29-50")
# ada摘要文本
ada_abs == "因此，为了在未来进行研究，本次研究也许能给学术界的其他学者带来建议"
# ada微调模型摘要文本

ada_ft_abs == """分布式防御措施的自动信任协商

面向自动信任协商的防御措施研究

自动信任协商的攻击面临"""
```

由于资费与效率，本次实验基于ada模型进行微调。可以看到，原始的ada模型几乎没有理解文本摘要任务的需求，只是在文本背景上生成了一段新的文本。在经过简单的微调后，ada模型已经有了质的飞跃，并且在一定程度上能够生成一个可用的摘要。不过，由于我们只使用500条数据

进行微调实验，模型的微调效果有限，生成的文本仍然远不及ChatGPT或其他在该任务上做过精细微调的大语言模型，如需进一步优化，可以增加训练样本并提高样本的质量，或者换一个更好的基础模型，这会增加一定的训练成本。

如果需要在一个微调模型上继续进行微调，直接将fine_tunes.create的-m参数改为微调后的模型名称即可，如下所示。
```bash
!openai api fine_tunes.create \
    -t "./dataset/csl_summarize_finetune_prepared.jsonl" \
    -m ada:ft-personal-2023-04-15-13-29-50\
    --no_check_if_files_exist
```
既可以通过fine_tunes.list查看所有微调模型，也可以通过openai.Model.list()查看名下所有可支持的模型，这里面包含了所有训练成功的微调模型。
```bash
!openai api fine_tunes.list
```
上面这条命令会输出一个模型信息列表，其中的每个元素是类似于如下示例的一个字典，其中包含了创建时间、模型名称、模型超参数、模型ID、基础模型名称、训练文件、执行状态等。每一个训练过的模型，不管训练成功还是失败，都会在这里展示出来。
```json
{
"created_at": 1681565036,
"fine_tuned_model": "ada:ft-personal-2023-04-15-13-29-50",
"hyperparams": {
"batch_size": 1,
"learning_rate_multiplier": 0.1,
"n_epochs": 4,
"prompt_loss_weight": 0.01
},
"id": "ft-LoKi6mOxIkOtFzcZTrmivKDa",
"model": "ada",
"object": "fine-tune",
(...)
}
```
查看可用的模型，其中包含自己微调的模型（它们以ft-personal开头）。
```python
models = openai.Model.list()
[x.id for x in models.data] == [
"babbage",
"davinci",
...,
"ada:ft-personal-2023-05-07-07-50-50",
"ada:ft-personal-2023-04-15-13-19-25",
"ada:ft-personal-2023-04-15-13-29-50"
]
```
如果需要删除自己微调的模型，可以使用openai.Model.delete命令。
```python
openai.Model.delete("ada:ft-personal-2023-04-15-12-54-03")
```
OpenAI的官方指引提供了很多与微调相关的参数与指令说明，感兴趣的读者可以从OpenAI官网获取更详细的指导。

### 4.3 文本纠错

#### 4.3.1 什么是文本纠错

在日常生活中，不管是微信聊天还是微博推文，甚至在图书中，我们都或多或少地发现过文本中的错别字现象。这些错别字可能源于语音输入时的口音偏差，如“飞机”被输入成了“灰机”；也可能是拼音输入时误触了临近键位或者选错了结果，如“飞机”被输入成了“得急”“肥鸡”；抑或是

手写输入时写成了形近字，如“战栗”被写成了“战粟”。常见的错误类型包括如下几种。

- 拼写错误：中文课程→中文砢碜，明天会议→明天会易。

- 语法错误：他昨天参加会议了→他昨天将要参加会议。

- 标点符号错误：您好，请多指教！→您好，请多指教？

- 知识性错误：上海黄浦区→上海黄埔区。

- 重复性错误：您好，请问您今天有空吗？→您好，请问您今天有空吗吗吗吗？

- 遗漏性错误：他昨天参加会议了→他昨天参加了。

- 语序性错误：他昨天参加会议了→他昨天会议参加了。

- 多语言错误：他昨天参加会议了→他昨天参加meeting了。


总之，文本错误可能千奇百怪。对于人类而言，凭常识与上下文，实现语义理解尚不是什么难事，有时只是些许影响阅读体验。但对于一些特定的文本下游任务，如命名实体抽取或意图识别，一条不加处理的错误输入文本，就可能导致南辕北辙的识别结果。

文本纠错指的是通过NLP技术对文本中出现的错误进行检测和纠正。它目前已经成为NLP领域的一个重要分支，被广泛地应用于搜索引擎、机器翻译、智能客服等各种场景。纵然由于文本错误的多样性，我们往往难以将所有错误通通识别并纠正成功，但是如果能尽可能多且正确地识别文

本中的错误，就能够极大降低人工审核的成本，不失为一桩美事。

#### 4.3.2 常见的文本纠错技术

常见的文本纠错技术主要有以下几种。

- 基于规则的文本纠错技术。

- 基于语言模型的文本纠错技术。

- 基于掩码语言模型（mask language model，MLM）的文本纠错技术。 

- 基于NLG的文本纠错技术。


1. **基于规则的文本纠错技术**

这种文本纠错技术通过实现定义的规则来检查文本中的拼写、语法、标点符号等常见错误。比如，“金字塔”常被误写为“金子塔”，可在数据库中加入两者的映射关系。这种传统方法由于需要大量的人力以及专家对于语言的深刻理解，因此难以处理海量文本或较为复杂的文本错误。


2. **基于语言模型的文本纠错技术**

基于语言模型的文本纠错技术包括错误检测和错误纠正，这种方法同样比较简单粗暴，速度快，扩展性强，但效果一般。常见的模型有KenLM。

- **错误检测**：使用结巴分词等分词工具对句子进行切词，然后结合字粒度和词粒度两方面得到疑似错误结果，形成疑似错误位置候选集。

- **错误纠正**：遍历所有的候选集并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度（一般来说，句子越通顺，困惑度越低），最后比较并排序所有候选集结果，得到最优纠正词。


3. **基于掩码语言模型的文本纠错技术**


BERT在预训练阶段执行了掩码语言模型和下一句预测（next sentence prediction，NSP）两个任务。其中掩码语言模型任务类似于英文的完形填空，在一段文本中随机遮住一个词，让模型通过上下文语境来预测这个词是什么；下一句预测任务则是给定两个句子，判断其中一个句子是

否为另一个句子的下一句，从而帮助模型理解上下文的语义连贯性。在BERT的后续改进模型中，RoBERTa将下一句预测直接放弃，ALBERT则将下一句预测替换成句子顺序预测（sentence order prediction，SOP）。这表明下一句预测任务作为一个分类任务，相对简单，BERT的主要能力

来源于掩码语言模型。

在掩码语言模型的训练阶段，有15%的词会被遮掩，而其中80%的词会被替换为[MASK]特殊标识，10%的词会被替换成随机的其他词，剩下10%的词保持不变。因此，总共有15%×10%的词会被替换为随机的其他词，从而迫使模型更多地依赖上下文信息来预测被遮掩的词，在一定程度上赋予模型纠错能力。

对BERT的掩码语言模型进行简单的修改，将输入设计为错误的词，输出为正确的词，做简单的微调，即可轻松实现文本纠错功能。比如Soft-Masked BERT模型就设计了一个二重网络来进行文本纠错，其中的“错误检测网络”通过一个简单的双向语言模型来判断每个字符出错的概率，“错

误纠正网络”则对出错概率更高的词进行遮掩，并预测出真实词。 

以下是一个基于Hugging Face的MacBERT4CSC进行纠错的样例。注意，MacBERT4CSC会自动将所有的英文字符转换为小写，我们在查看修改时需要忽略大小写方面的差异。
```python
from transformers import BertTokenizer, BertForMaskedLM

# 载入模型
tokenizer = BertTokenizer.from_pretrained("shibing624/macbert4csc-base-chinese")
model = BertForMaskedLM.from_pretrained("shibing624/macbert4csc-base-chinese")

text = "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课乘吧！"
input_ids = tokenizer([text], padding=True, return_tensors="pt")

# 生成结果文本
with torch.no_grad():
    outputs = model(*input_ids)
output_ids = torch.argmax(outputs.logits, dim=-1)
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(" ", "")
# 纠错文本
output_text == "大家好，一起来参加datawhale的“chatgpt使用指南”组队学习课程吧！"
```

进一步地，我们可以通过以下脚本来展示修改的位置。

```python
import operator

def get_errors(corrected_text, origin_text):
    sub_details = []
    for i, ori_char in enumerate(origin_text):
        if ori_char in [" ", "“", "”", "'", "‘", "琊", "\n", "...", "—", "擤"]:
            # 过滤特殊字符差异
            corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]
            continue
        if i >= len(corrected_text):
            continue
        if ori_char != corrected_text[i]:
            if ori_char.lower() == corrected_text[i]:
                # 过滤大小写差异
                corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]
                continue
            sub_details.append((ori_char, corrected_text[i], i, i + 1))
    sub_details = sorted(sub_details, key=operator.itemgetter(2))
    return corrected_text, sub_details

correct_text, details = get_errors(output_text[:len(text)], text)
details == [("乘", "程", 37, 38)]
```

4. **基于NLG的文本纠错技术**

上面提到的基于掩码语言模型的方法只能用于输入与输出等长的情况，但实际应用中往往会出现输入与输出不等长的情况，如错字或多字。一种可能的解决办法是，在原有的BERT模型后嵌入一层Transformer解码器，也就是将文本纠错任务等价成“将错误的文本翻译成正确的文本”任务。

不过，此时我们没法保证输出文本与原始文本中正确的部分一定能够保持完全一致，这可能会在语义不变的情况下，生成一种新的表达方式。

#### 4.3.3 基于OpenAI接口的文本纠错实验

我们直接尝试使用ChatGPT来进行文本纠错，如下所示。

```python
def correct_text(text):
    content = f"请对以下文本进行文本纠错：\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": content}]
    )
    corrected_text = response.get("choices")[0].get("message").get("content")
    return corrected_text

text = "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课乘吧！"
output_text = correct_text(text)
# 纠错文本
output_text == "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习
