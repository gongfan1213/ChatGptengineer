想要的内容，不输出我们不想要的内容。可控文本生成的方法有很多，由于涉及模型训练或微调，因此我们不过多深入介绍，感兴趣的读者可以查找并阅读相关文献。简单来看，可控文本生成的方法主要分为三类。

- **使用控制Token**：在文本的开头增加一个控制生成属性的Token，这里的属性可以是情感倾向、主题、风格等。基于提示词的控制生成也可以算作这一类型，尤其是当模型经过带控制的提示词微调训练后。 

- **使用控制模型**：主要体现在生成过程中，使用一个或多个属性分类器对生成过程进行引导。一般做法是保持大语言模型的参数不变，更新分类器的参数，使其能够判别不同属性，或者区分想要的内容和不想要的内容。生成时使用分类器作为条件，以影响大语言模型输出时Token的概率分布。 

- **使用反馈控制**：典型的代表就是RLHF，比如InstructGPT，将有帮助、真实性和无害性作为反馈标准影响模型。此外，类似自训练、自学习的方法也可以被看作从反馈中学习，这里的反馈可能来自人类，也可能来自模型。这类方法从根本上改变了模型，是彻底的控制。 

对于安全问题，虽然本节已经给出了不少防范措施，但我们仍然不建议让大语言模型直接面向用户。我们实在难以保证其中不出差错，而一旦出差错，带来的风险就是巨大的。同时，我们建议在设计上考虑以下辅助方案。

- 建议增加消息撤回机制。即使大语言模型发送了一些风险内容，但只要及时撤回，在一定程度上也能将风险降到最低。 

- 建议对用户账号进行严格管控，如果有用户触发风险内容，应及时予以关注。对刻意发送或诱导大语言模型输出风险内容的账号，应马上对其进行限制。 

- 留存所有的对话和消息记录，以备事后查验。 

考虑服务商可能会提供一些关于安全检查的最佳实践，建议读者在开发前仔细阅读，尽量遵循文档进行处理。

### 6.3 网络：接口调用并不总是成功

由于本书涉及的大语言模型都是通过接口来使用的，因此避免不了网络请求。本节主要介绍和网络请求相关的实践。


#### 6.3.1 失败
网络请求失败是常见的情况，只要服务代码中有需要通过网络请求第三方接口的，都应该关注这个问题。对于网络请求失败的情况，常用的解决方法是重试。是的，当本次接口调用失败或超时时，应再次发起请求。不过，重试并不是简单地只要失败就重新发起请求，这里有非常多的细节需要考虑。

- **哪些情况需要重试**：在调用接口时，并不是对所有的失败请求均进行重试，比如访问的令牌到期、服务端返回未鉴权错误等，此时应该调用鉴权接口以重新获取令牌，而不是一直重试。一般来说，我们可以对网络超时、服务端错误等导致的失败请求进行重试。 

- **多久重试一次**：连续的重试肯定是不合理的，这可能导致服务端响应更慢。一般情况下，可以随时间的增加而相应地增加重试间隔时间，比如常用的指数级增加重试间隔时间，第一次间隔2秒，第二次间隔4秒，第三次间隔8秒，以此类推。 

- **重试多少次停止**：可以肯定的是，我们不可能一直重试下去，若一个接口重试几次后依然失败，则说明服务或网络发生了故障。在这种情况下，重试多少次都是没有用的。可以根据实际场景选择相应的配置，一般最多重试3次即可。 

若经过以上重试策略后依然失败，则抛出异常，有可能同时向客户端返回预先指定的结果，也有可能回调某个专门处理接口调用失败的函数。这些都应该根据实际需要进行设计。

重试策略看起来还不错，不过，让我们考虑一种特殊情况。假设第三方接口或网络真的发生了故障，此时，客户端一直在发请求，而服务端在重试策略下不停地重试，结果自然是每个请求都达到重试上限并最终抛出异常。

对于这种情况，我们一般会加入熔断机制。当失败次数达到某个阈值时，对服务进行熔断，直接返回预设好的响应或者干脆拒绝请求。这样后面的请求就不会再调用第三方接口了。在实践中，我们往往会返回一个简化版的处理结果。这 

被称为服务降级。

熔断机制也经常主动用在高峰限流场景下，当某个时刻请求突然暴增导致资源不够用时，可以有规划地对一些不重要的服务进行熔断。比如做“秒杀”活动时，为了保证短时间涌入的大量请求能够得到响应，可以对商品查询、筛选等功能返回缓存或上一次查询的结果。

熔断机制尤其适合一个接口包含多个请求源，最终返回整合结果的情况。此时，熔断有问题的请求源，可以保证整个接口依然可用。熔断一段时间后，可以尝试自动恢复，先放行一定数量的请求。如果响应成功，则关闭熔断，否则继续对有问题的请求源保持熔断状态。

最后，我们强烈建议对服务增加可视化的配置和监控，同时启用告警功能，以便当任何一个服务出现问题时，能够及时进行干预，保证服务稳定可用。

### 6.3.2 延迟
延迟是指接口没有在指定时间内给出响应，但又不会超时失败的情况。延迟比失败更加常见，它不仅和当时的网络状况有关，也和网络配置（比如带宽、是否有专用网络等）有关，还和接口功能的复杂度，以及请求和返回的数据量等非网络因素有关。对于大语言模型接口的延迟，我们给出以下实践建议。

首先，建议针对不同的需求选择不同规模的模型。以OpenAI为例，它提供了多个不同版本的模型，读者可以根据任务难度选择适当的模型。越复杂的模型，响应速度越慢，不仅耗时较长，价格还贵。总之，还是我们一直强调的观点，工具只是手段，不是目的，能用简单的低成本方案解决的就不要用复杂的高成本方案。

其次，大语言模型接口一般会提供“停止序列”参数，读者可以关注并配置该参数，以便及时结束模型的输出。同时，我们在6.2节中也提到过，可以在提示词中限制输出的Token数，让答案尽量简短。模型输出的内容越少，响应时间越快，延迟越低。当然，接口的请求体（request body）也不应该过大，太长的上下文不仅会增加传输时间，还可能会增加费用（以OpenAI为例，提示词也是要计费的）。这可能需要使用一些语义匹配或信息压缩提炼技术，每次只传输最相关的上下文，第2章和第3章对此有所涉及，此处不再展开讨论。 

再次，针对部分场景应用，可以使用流式输出。流式输出体验比较好，用户很少会感到延迟，因为模型接口一旦接收到请求就开始响应。目前比较常用的服务端方案包括SSE（server - sent event）和WebSocket。SSE是一种基于HTTP的单向通信技术，允许服务端向客户端发送持续的事件流。WebSocket作为一种全双工通信技术，允许客户端和服务端建立双向通信通道。SSE更加适合服务端向客户端持续发送数据的情况，而WebSocket则更加适合客户端和服务端实时交互（如对话）的情况。 

最后，考虑使用缓存。这在某些场景下是可以的，比如问答类应用。事实上，只要每次交互的上下文不是动态变化的，就可以考虑使用缓存。 

以上是关于降低大语言模型接口延迟的通用建议，在实际场景中，如果遇到延迟问题，建议读者仔细分析服务代码，找到关键瓶颈进行解决。尤其是当接口中包含比较多的网络请求时，任何一个网络请求都有可能成为瓶颈，比如当历史文档非常庞大时，使用向量库或数据库查询相关上下文也会比较耗时。

### 6.3.3 扩展
本书迄今为止一直未提到高并发场景，也默认用户请求不太多，单个账号就可以提供服务，这在现实中是存在的，尤其是新产品或服务还没有很多用户的时候。但对于一款成功的商业应用来说，用户一定不会少，并发也会比较高。此时，对接口服务进行扩展就是我们重点所要考虑的事项。需要说明的是，这里主要指横向扩展。 

当决定对服务进行扩展时，首先要做的就是了解基本情况和需求，包括日均调用次数、日调用峰值、平均并发数、最大并发数、期望平均响应时长、是否可以使用缓存等。除此之外，还需要了解大语言模型服务商的相关政策，比如OpenAI对接口调用就有限制，不同模型、不同类型账号，限制也不一样。值得说明的是，服务提供商一般会提供关于扩展的最佳实践，这也是我们重点需要掌握的信息。这些信息对于我们接下来的方案至关重要，我们期望能够以最低的成本满足业务或用户需要。

了解完基本情况，就可以对需要的资源和成本进行大致估算了。资源主要就是账号，建议最好有20%以上的冗余。成本可以通过提示词和响应的平均长度，以及日均调用次数来进行简单估算。 

对于账号资源，一般需要构建一个资源池来进行统一管理。当需要调用时，先从资源池中选择一个可用的账号完成本次调用。如果需要的账号比较多，或者预估未来的调用次数会增长，则建议一开始就把资源池模块写好，以便日后自由扩展。 

资源池模块应该具备基本的添加、删除、修改功能，建议构建适合企业内部的自定义账号体系，将真实的账号绑定在自定义账号上。这样做的好处是不仅便于管理，也方便支持多个不同的大语言模型服务商。资源池模块还有一项基本功能——报表统计，包括从不同维度统计调用方、调用次数、失败率、费用等。 

当然，对资源池模块来说，最重要的功能还是资源调度，简单来说，就是如何为每一次调用分配账号。我们应该支持使用多种不同的策略进行调度，比如简单地随机选择账号、按失败率从低到高、按不同功能使用不同服务商等。需要特别注意的是，不要把任何与业务相关的东西掺杂进来，资源池模块只负责管理资源。 

大语言模型（或其他模型）接口往往还支持批量（batch）模式，也就是一次发送多条请求，同时获取这些请求的响应。这在并发比较高、响应时间又不要求那么紧的情况下非常适合。如果是流式输出，响应时间这一项可以忽略，此时建议使用批量模式。 

使用批量模式需要在用户请求和请求大语言模型服务商接口之间做一层处理，合并用户请求，批量地一次性向大语言模型服务商发起请求，收到反馈后分发到对应的用户请求响应上。具体来说，我们可以维护一个队列和最小请求间隔时间，在最小请求间隔时间内，固定窗口大小的请求同时出队进行批量请求。批量大小理论上是不受限制的（这意味着可以不固定窗口大小），但建议最好选择一个合适的最大窗口值，这样既能达到最大的吞吐量，又不至于有太高的延迟。注意，这里是“最大窗口值”，因为当并发很低时，队列内不一定有达到最大窗口值的请求数量。 

如果可以的话，每个窗口的大小最好是2的指数（1、2、4、8、16、32等），因为服务器往往是GPU，这样的处理能提升一些效率。当然，大部分服务商应该已经在服务内部做了类似的优化，批量大小即使不是2的指数，性能差别也几乎可以忽略。 

刚才我们提到了使用队列，事实上，强烈建议将服务端写成队列模式，这样做的好处是，当资源不足时，服务依然是可用的，除非资源和请求数量严重不均衡，这就意味着我们不需要按照并发的峰值申请资源。如果对服务的响应时间要求没那么高，用户能接受偶尔出现的一定时间的等待，则这种方式可以极大地节约资源。


### 6.4 本章小结
开发一个演示用的小应用和实现真正可商用的服务落地之间有非常多的事情要处理。本章从评测开始介绍，让你了解到一个模型服务先得“能用”，之后才能上线，我们介绍了针对不同任务的评测方法。接下来，我们强调了安全对于一个大语言模型相关应用或服务的重要性，并给出了一些处理方案。最后，针对大语言模型服务接口调用，我们给出了如何更好地构建稳定、可靠服务的建议。本章内容都针对真实场景下的应用和服务构建，虽然我们给出了一些工程实践经验，但读者应该清楚，现实中遇到的问题可能远比我们提到的多。而且，我们并未涉及超大规模分布式处理，这是有意为之。如果你或你所在的企业已经达到这种程度，则一定会有精通算法工程和架构的同事专门对此负责。总之，真实场景下的工程实践需要考虑更多的因素，并且需要更加仔细地进行设计，处处充满权衡的艺术。 
