### 第3章 句词分类——句子Token都是类别

第2章介绍了相似匹配的基础知识，以及使用相似匹配技术能够实现的任务和应用。相似匹配以Embedding为核心，关注的是如何更好地表示文本。基于Embedding的表示往往是语义层面的，一般使用余弦相似度来衡量。我们也提到了，其实不光文本可以Embedding，任意对象都可以Embedding，这一技术已被广泛应用于深度学习算法的各个领域。

本章关注NLP领域最常见的两类任务——句子分类和Token分类，由于中文里的字也是词，因此这两类又叫作句词分类。我们将首先介绍句词分类的基础知识，包括相关的一些常见任务，以及如何对句子和Token进行分类。接下来介绍ChatGPT相关接口的用法，其他厂商提供的类似接口的用法也类似。通过类似ChatGPT这样的大语言模型接口其实可以做很多任务，句词分类只是其中一部分。最后介绍相关的应用，这些应用的问题使用传统方法也可以解决，但一般会更加复杂、更加麻烦。相比较而言，基于大语言模型就简单多了，而且效果也不错。与第2章一样，我们依然重点关注最终目的以及达到最终目的所使用的方法与流程。



#### 3.1 句词分类基础

自然语言理解（natural language understanding，NLU）任务与自然语言生成（natural language generation，NLG）任务并称NLP两大主流任务。一般意义上的NLU任务指的是与理解给定句子的意思相关的情感分析、意图识别、实体抽取、关系抽取等任务，在智能对话中应用比较广泛。具体来说，当用户输入一句话时，机器人一般会针对这句话（也可以把历史记录给附加上）进行全方面分析，包括但不限于以下内容。

- **情感分析**：简单来说，情感分析一般包括正向、中性、负向三种类型，也可以设计更多的类别或更复杂的细粒度情感分析。我们着重讲一下细粒度情感分析，它主要针对其中某个实体或属性的情感。比如在电商购物的商品评论中，用户可能会对商品价格、快递、服务等一个或多个方面表达自己的观点。这时候，我们更加需要的是对不同属性的情感倾向，而不是整个评论的情感倾向。 

- **意图识别**：意图识别一般用分类模型：大部分是多分类，但也有可能是层级标签分类或多标签分类。多分类是指给定输入文本，输出一个标签，但可以使用的标签有多个，比如对话文本的标签可能包括询问地址、询问时间、询问价格、闲聊等。层级标签分类是指给定输入文本，输出层级的标签，也就是从根节点到最终细粒度类别的路径，比如询问地址或询问家庭地址、询问地址或询问公司地址等。多标签分类是指给定输入文本，输出不定数量的标签，也就是说，每个文本可能有多个标签，标签之间是平级关系，比如投诉、建议（在投诉的同时提出了建议）。 

- **实体和关系抽取**：实体抽取就是提取出给定文本中的实体。实体一般指具有特定意义的实词，如人名、地名、作品、品牌等，大部分是与业务直接相关或需要重点关注的词。关系抽取是指实体与实体之间的关系判断。实体之间往往有一定的关系，比如“中国四大名著之一《红楼梦》由清代作家曹雪芹编写。”其中“曹雪芹”是人名，“红楼梦”是作品名，其中的关系就是“编写”，一般与实体作为三元组来表示：(曹雪芹，编写，红楼梦)。 

经过上面这些分析后，机器人就可以对用户的输入有一个比较清晰的理解，便于接下来据此做出响应。另外值得一提的是，上面的过程并不一定只用在对话中，只要涉及用户输入查询，需要系统给出响应的场景，都需要这个自然语言理解的过程，一般也叫该过程为Query解析。 

上面的一些分类，如果从算法的角度看，可以分为如下两种。

- **句子分类**：如情感分析、意图识别、关系抽取等，也就是给一个句子（也可能有其他一些信息），给出一个或多个标签。 

- **Token分类**：如实体抽取、阅读理解（给定一段文本和一个问题，然后从文本中找出问题的答案），也就是给一个句子，给出对应实体或答案的索引位置。 



Token分类不太好理解，我们举个例子，比如刚刚提到的一句话——“中国四大名著之一《红楼梦》由清代作家曹雪芹编写。”它在标注的时候是以下这样的。

| 中 | O |
| 国 | O |
| 四 | O |
| 大 | O |
| 名 | O |
| 著 | O |
| 之 | O |
| 一 | O |
| 《 | O |
| 红 | B-WORK |
| 楼 | I-WORK |
| 梦 | I-WORK |
| 》 | O |
| 由 | O |
| 清 | O |
| 代 | O |
| 作 | O |
| 家 | O |
| 曹 | B-PERSON |
| 雪 | I-PERSON |
| 芹 | I-PERSON |
| 编 | O |
| 写 | O |
| 。 | O |

在这个例子中，每个Token就是每个字，每个Token对应一个标签（当然也可以对应多个标签）。标签中的B表示开始（Begin）；I表示内部（Internal）；O表示其他（Other），也就是非实体。“红楼梦”是作品，我们标注为WORK；“曹雪芹”是人名，我们标注为PERSON。当然，也可以根据实际需要决定是否标注“中国”“清代”等实体。模型要做的就是学习这种对应关系，当给出新的文本时，要能够给出每个Token的标签预测。 

可以看到，它们在本质上都是分类任务，只是分类的位置或标准不一样。当然，实际应用中会有各种不同的变化和设计，但整个思路是差不多的，我们并不需要掌握其中的细节，只需要知道输入输出和基本的逻辑就可以了。

#### 3.1.1 如何对一句话进行分类

接下来，我们简单介绍这些分类具体是怎么做的，先说句子分类。回顾第2章的内容，Embedding是整个深度学习NLP的基石，本小节的内容也会用到Embedding，具体过程如下。

- 将给定句子或文本表示成Embedding。 

- 将Embedding传入一个神经网络，计算得到不同标签的概率分布。 

- 将得到的标签概率分布与真实的标签做比较，并将误差回传，修改神经网络的参数，即训练。 

- 得到训练好的神经网络，即模型。 


举个例子，为简单起见，我们假设Embedding的维度为32维（OpenAI返回的维度比较大，见第2章），如下所示。

```python
import numpy as np
np.random.seed(0)
emd = np.random.normal(0, 1, (1, 32))
```

在这里，我们随机生成一个均值为0、标准差为1的1×32维的高斯分布作为Embedding数组，维度为1表示词表中只有一个Token。如果是三分类，那么最简单的模型参数W的大小就是32×3，模型的预测过程如下。

```python
W = np.random.random((32, 3))
z = emd @ W
z == array([[6.93930177, 5.96232449, 3.96168115]])
z.shape == (1, 3)
```

这里得到的z一般被称为logits。如果想要概率分布，则需要对其进行归一化，也就是将logits变成介于0和1之间的概率值，并且每一行加起来为1（即100%），如下所示。

```python
def norm(z):
    exp = np.exp(z)
    return exp / np.sum(exp)
y = norm(z)
y == array([[0.70059356, 0.26373654, 0.0356699 ]])
np.sum(y) == 0.9999999999999999 
```

根据给出的y，我们得知预测的标签是第0个位置的标签，因为那个位置的概率最大（约70.06%）。如果真实的标签是第1个位置的标签，那么第1个位置的标签实际就是1（100%），但目前预测的第1个位置的概率只有大约26.37%，将这个误差回传以调整模型参数W。下次计算时，第0个位置的概率就会变小，第1个位置的概率则会变大。这样通过标注的数据样本不断循环迭代的过程，其实就是模型训练的过程，也就是通过标注数据，让模型尽可能正确地预测出标签。 

在实践中，模型参数W往往比较复杂，可以包含任意的数组，只要最后的输出变成1×3的大小即可，我们举个稍微复杂点的例子。

```python

w1 = np.random.random((32, 100))
w2 = np.random.random((100, 32))
w3 = np.random.random((32, 3))
y = norm(norm(norm(emd @ w1) @ w2) @ w3)
y == array([[0.32940147, 0.34281657, 0.32778196]])
```

可以看到，现在有三个数组的模型参数，形式上虽然复杂了些，但结果是一样的，依然是一个1×3大小的数组。接下来的过程就和前面一样了。

稍微复杂点的是多标签分类和层级标签分类，它们因为输出的都是多个标签，所以处理起来要麻烦一些，不过它们的处理方式是类似的。我们以多标签分类来说明。假设有10个标签，给定输入文本，可能是其中任意多个标签。这就意味着我们需要将10个标签的概率分布都表示出来。可以针对每个标签做二分类，也就是说，输出的大小是10×2，每一行表示“是否为该标签”的概率分布，示例如下。

```python
def norm(z):
    axis = -1
    exp = np.exp(z)
    return exp / np.expand_dims(np.sum(exp, axis=axis), axis)
np.random.seed(42)
emd = np.random.normal(0, 1, (1, 32))
W = np.random.random((10, 32, 2))
y = norm(emd @ W)
y.shape == (10, 1, 2)
y == array([
       [[0.66293305, 0.33706695]],
       [[0.76852603, 0.23147397]],
       [[0.59404023, 0.40595977]],
       [[0.04682992, 0.95317008]],
       [[0.84782999, 0.15217001]],
       [[0.01194495, 0.98805505]],
       [[0.96779413, 0.03220587]],
       [[0.04782398, 0.95217602]],
       [[0.41894957, 0.58105043]],
       [[0.43668264, 0.56331736]]
   ])
```
这里的输出中的每一行有两个值，分别表示“不是该标签”和“是该标签”的概率。比如第一行，预测结果显示，不是该标签的概率约为66.29%，是该标签的概率约为33.71%。需要注意的是，在进行归一化时，必须指定维度求和。 

则就变成所有的概率值加起来为1了，这就不对了（应该是每一行的概率和为1）。 

以上是句子分类的逻辑，我们必须再次说明，实际场景会比这里的例子复杂得多，但基本思路是一样的。在大语言模型时代，我们并不需要自己构建模型，本章后面会讲到如何使用大语言模型接口执行各类任务。

#### 3.1.2 从句子分类到Token分类

接下来我们再看看Token分类，有了刚才的基础，Token分类就比较容易理解了。Token分类的最大特点是，Embedding是针对每个Token的。也就是说，如果给定文本的长度为10，并且假定维度依然是32，那么Embedding的大小为(1, 10, 32)，相比句子分类用到的(1, 32)多了一个10。换句话说，这个文本的每一个Token都是一个32维的向量。 

下面我们假设标签共有5个，和前面的例子对应，分别为B-PERSON、I-PERSON、B-WORK、I-WORK和O。基本过程如下。

```python
emd = np.random.normal(0, 1, (1, 10, 32))
W = np.random.random((32, 5))
z = emd @ W
y = norm(z)
y.shape == (1, 10, 5)
y == array([[
    [0.23850186, 0.04651826, 0.12495322, 0.28764271, 0.30238395],
    [0.06401011, 0.34220550, 0.54911626, 0.01179874, 0.03286939],
    [0.18309536, 0.62132479, 0.09037235, 0.06016401, 0.04504349],
    [0.01570559, 0.02714370, 0.20159052, 0.12386611, 0.63169408],
    [0.13085410, 0.06810165, 0.61293236, 0.00692553, 0.18118637],
    [0.08011671, 0.04648297, 0.00200392, 0.02913598, 0.84226041],
    [0.05143706, 0.09635837, 0.001115594, 0.83118412, 0.01986451],
    [0.03721064, 0.14529403, 0.03049475, 0.76177941, 0.02522117],
    [0.24154874, 0.28648044, 0.11024747, 0.35380566, 0.00791770],
    [0.10965428, 0.00432547, 0.08823724, 0.00407713, 0.79370588]
]])
```

注意看，每一行表示一个Token是某个标签的概率分布（每一行加起来为1），比如第一行：

```python
sum([0.23850186, 0.04651826, 0.12495322, 0.28764271, 0.30238395]) == 1.00000000
```

这里具体的意思是，第一个Token是第0个位置的标签（如果标签按上面给出的顺序，那就是B-PERSON）的概率约为23.85%，其他类似。根据这里预测的结果，第一个Token的标签是O，真实的标签和这个预测的标签之间可能有误差，通过误差就可以更新参数，从而使得之后预测时能预测到正确的标签（也就是正确位置的概率最大）。不难看出，这个逻辑和前面的句子分类是类似的，其实就是对每一个Token做了多分类。 

关于NLU常见问题的基本原理我们就介绍到这里，读者如果对更多细节感兴趣，可以阅读与NLP算法相关的参考资料，从一个可直接上手的小项目开始，一步一步构建自己的知识体系。

#### 3.2 ChatGPT接口使用

#### 3.2.1 基础版GPT续写

本小节介绍OpenAI的Completion接口，以及如何利用大语言模型的In-Context学习能力进行零样本或少样本的推理。这里有几个重要概念，我们简要回顾一下。

- **In-Context**：简单来说就是一种上下文学习能力，也就是说，模型只要根据输入的文本就可以自动给出对应的结果。这种能力是大语言模型在学习了非常多的文本后获得的，可以被看作一种内在的理解能力。 

- **零样本**：直接给模型文本，让它给出我们想要的标签或输出。 

- **少样本**：给模型一些类似的样例（输入+输出），再拼上一个新的没有输出的输入，让模型给出输出。 


接下来，我们就可以使用同一个接口，通过构造不同的输入来完成不同的任务。换句话说，通过借助大语言模型的In-Context学习能力，我们只需要在输入的时候告诉模型我们的任务就行，让我们来看看具体的用法。

```python
import openai
import os
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
def complete(prompt: str) -> str:
    response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompt,
        temperature=0,
        max_tokens=64,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=0.0
    )
    ans = response.choices[0].text
    return ans
```

Completion接口不仅能帮助我们完成一段话或一篇文章的续写，而且可以用来执行各种各样的任务，比如本章介绍的句子分类和实体抽取任务。相比Embedding接口，Completion接口的参数要复杂得多，下面我们对其中比较重要的参数进行说明。

- **model**：模型，text-davinci-003就是一个模型，我们可以根据自己的需要，参考官方文档进行选择，一般需要综合价格和效果进行权衡。 

- **prompt**：提示词，默认为<|endoftext|>，它是模型在训练期间看到的文档分隔符。因此，如果未指定提示词，模型将像从新文档开始一样。简单来说，prompt就是给模型的提示词。 

- **max_tokens**：生成的最大Token数，默认为16。注意，这里的Token数不一定是字数。提示词+生成的文本，所有的Token长度都不能超过模型的上下文长度。不同模型可支持的最大长度不同，可参考相应文档。 

- **temperature**：温度，默认为1。采样温度介于0和2之间。较高的值（如0.8）将使输出更加随机，而较低的值（如0.2）将使输出更加集中和确定。通常建议调整这个参数或下面的top_p，但不建议同时调整两者。 

- **top_p**：下一个Token在累积概率为top_p的Token中采样。默认为1，表示所有Token在采样范围内，0.8则意味着只选择前80%概率的Token进行下一次采样。 

- **stop**：停止的Token或序列，默认为null，最多4个，如果遇到停止的Token或序列，就停止继续生成。注意生成的结果中不包含stop。 

- **presence_penalty**：存在惩罚，默认为0，取值介于 -2.0和2.0之间。正值会根据新Token到目前为止是否出现在文本中来惩罚它们，从而增大模型讨论新主题的可能性，值太高则可能会降低样本质量。 

- **frequency_penalty**：频次惩罚，默认为0，取值介于 -2.0和2.0之间。正值会根据新Token到目前为止在文本中的现有频率来惩罚新Token，减小模型重复生成相同内容的可能性，值太高则可能会降低样本质量。 


在大部分情况下，我们只需要考虑上面几个参数即可，甚至只需要考虑前两个参数。不过，熟悉上面的参数能帮助我们更好地使用接口。另外值得说明的是，虽然这里用的是OpenAI的接口，但其他类似接口的参数也差得不太多。请熟悉这里的参数，到时候切换起来便能得心应手。 

下面我们先看几个句子分类的例子，我们将分别展示怎么使用零样本和少样本。零样本的例子如下所示。

```python
# 零样本，来自OpenAI官方示例
prompt = """
The following is a list of companies and the categories they fall into:
Apple, Facebook, FedEx
Apple
Category:
"""
ans = complete(prompt)
ans == """
Technology
Facebook
Category:
Social Media
Fedex
Category:
Logistics and Delivery
"""
```

可以看到，我们只列出了公司名称和对应的格式，模型可以返回每个公司所属的类别。下面是少样本的例子。

```python
# 少样本
prompt = """今天真开心。 --> 正向
心情不好。 --> 负向
我们是快乐的年轻人。 -->
"""
ans = complete(prompt)
ans == """
正向
"""
``` 
