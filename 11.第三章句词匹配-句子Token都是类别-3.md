### 3.3 相关任务与应用

#### 3.3.1 文档问答：给定文档问问题

文档问答任务和QA任务有点类似，不过文档问答任务要稍微复杂一点。首先用QA任务的方法召回一个相关的文档，然后让模型在这个文档中找出问题的答案。一般的流程还是先召回相关文档，再做阅读理解。阅读理解和实体抽取任务有些类似，但前者预测的不是具体某个标签，而是答案在原始文档中的位置索引，即开始和结束的位置。 

举个例子，假设我们的问题是：“北京奥运会举办于哪一年？”召回的文档可能含有北京奥运会举办的新闻，比如下面这个文档。其中，“2008年”这个答案在文档中的索引就是标注数据时所要标注的内容。

“第29届夏季奥林匹克运动会（Beijing 2008；Games of the XXIX Olympiad），又称2008年北京奥运会，2008年8月8日晚上8时整在中国首都北京开幕，8月24日闭幕。”

当然，一个文档里可能有不止一个问题，比如上面的文档，还可以问：“北京奥运会什么时候开幕？”“北京奥运会什么时候闭幕？”“北京奥运会是第几届奥运会？”等。 

根据之前的NLP方法，这个任务实际做起来方案比较多，也有一定的复杂度，不过总体来说还是语义匹配和Token分类任务。现在我们有了大语言模型，问题就变得简单了。依然是两步，如下所示。

- **第一步**：召回相关文档。与QA任务类似，但这里召回的不是问题，而是文档，即计算给定问题与一批文档的相似度，从中选出相似度最高的那个文档。 

- **第二步**：基于给定文档回答问题。将召回的文档和问题以提示词的方式提交给大语言模型接口（比如之前介绍的Completion和ChatCompletion接口），直接让大语言模型帮忙给出答案。 



第一步我们已经比较熟悉了，对于第二步，我们分别用两个不同的接口各举一例。首先看看Completion接口，如下所示。

```python
import openai
import os
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
def complete(prompt: str) -> str:
    response = openai.Completion.create(
        prompt=prompt,
        temperature=0,
        max_tokens=300,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        model="text-davinci-003"
    )
    ans = response["choices"][0]["text"].strip("\n")
    return ans
```

假设第一步已经完成，我们得到了一篇文档。注意，这篇文档一般会比较长，所以提示词也会比较长，如下所示。

```python
# 来自OpenAI官方示例
prompt = """Answer the question as truthfully as possible using the 
provided text, and if the answer is not contained within the text below, 
say "I don't know"
Context:
The men's high jump event at the 2020 Summer Olympics took place between 30 
July and 1 August 2021 at the Olympic Stadium. 33 athletes from 24 nations 
competed; the total possible number depended on how many nations would use 
universality places to enter athletes in addition to the 32 qualifying 
through mark or ranking (no universality places were used in 2021). Italian 
athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim 
emerged as joint winners of the event following a tie between both of them 
as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold 
medal in a rare instance where the athletes of different nations had agreed 
to share the same medal in the history of Olympics. Barshim in particular 
was heard to ask a competition official "can we have two golds?" in response 
to being offered a 'jump off'. Maksim Nedasekau of Belarus took bronze. The 
medals were the first gold in the men's high jump for Italy and Qatar, and the 
third consecutive medal in the men's high jump for Belarus (all by Barshim).
Q: Who won the 2020 Summer Olympics men's high jump?
A:"""
ans = complete(prompt)
ans == "Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners 
of the event."
```

上面的Context就是我们召回的文档。可以看到，Completion接口很好地给出了答案。另外需要说明的是，我们在构造提示词时其实还给出了一些限制，主要包括两点：第一，要求根据给定的文本尽量真实地回答问题；第二，如果答案未包含在给定文本中，就回复“我不知道”。这些都是为了尽量保证输出结果的准确性，减小模型胡言乱语的可能性。 

接下来看ChatCompletion接口，我们选择一个中文的例子，如下所示。

```python
prompt = """请根据以下Context回答问题，直接输出答案即可，不用附带任何上下文。
Context:
尤金袋鼠（Macropus eugenii）是袋鼠科中细小的成员，通常都是就袋鼠及有袋类的研究对象。尤金袋鼠分布在澳洲南部岛屿及西岸地区。它们每季在袋鼠岛都会大量繁殖，破坏了针鼹岛上的生活环境而被认为是害虫。尤金袋鼠最初是于1628年船难的生还者在西澳发现的，是欧洲人最早有纪录的袋鼠发现，且可能是最早发现的澳洲哺乳动物。尤金袋鼠共有三个亚种。尤金袋鼠很细小，约只有8公斤重，适合饲养。尤金袋鼠的奶中有一种物质，被称为AGG01，有可能是一种神奇药，青霉素的改良版。AGG01是一种蛋白质，在实验中证实比青霉素有效100倍，可以杀死99%的细菌及真菌，如沙门氏菌、变形杆菌及金黄色葡萄球菌。
问题:
尤金袋鼠分布在哪些地区?
"""
def ask(content: str) -> str:
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": content}]
    )
    ans = response.get("choices")[0].get("message").get("content")
    return ans
ans = ask(prompt)
ans == "尤金袋鼠分布在澳洲南部岛屿及西岸地区。"
```

看起来没什么问题。下面就以Completion接口为例把两个步骤串起来。 

我们使用OpenAI提供的数据集——来自维基百科的关于2020年东京奥运会的数据。该数据集可以从OpenAI的openai-cookbook的GitHub仓库“examples/fine-tuned_qa/”获取。下载后是一个CSV文件，和之前一样，先加载并查看数据集。

```python
import pandas as pd
df = pd.read_csv("./dataset/olympics_sections_text.csv")
df.shape == (3964, 4)
df.head()
```

数据集中的前5条如表3-1所示，数据集中的第一列是页面标题，第二列是章节标题，第三列是章节内容，最后一列是Token数。

| 索引 | 页面标题 | 章节标题 | 章节内容 | Token数 |
| ---- | ---- | ---- | ---- | ---- |
| 0 | 2020 Summer Olympics | Summary | The 2020 Summer Olympics (Japanese: 2020年夏季オリンピック ... | 726 |
| 1 | 2020 Summer Olympics | Host city selection | The International Olympic Committee (IOC) vote... | 126 |
| 2 | 2020 Summer Olympics | Impact of the COVID-19 pandemic | In January 2020, concerns were raised about th... | 374 |
| 3 | 2020 Summer Olympics | Qualifying event cancellation and postponement | Concerns about the pandemic began to affect qu... | 298 |
| 4 | 2020 Summer Olympics | Effect on doping tests | Mandatory doping tests were being severely res... | 163 |


在这里，我们把数据集中的第三列作为文档，基本流程如下。

- **第1步**：对每个文档计算Embedding。 

- **第2步**：存储Embedding，同时存储内容及其他需要的信息（如章节标题）。 

- **第3步**：从存储的地方检索最相关的文档。 

- **第4步**：基于最相关的文档回答给定的问题。 



上面的第1步依然需要借助OpenAI的Embedding接口，但是第2步我们这次不用Redis，而是换用一个向量搜索工具——Qdrant。Qdrant相比Redis更简单易用且容易扩展。不过，我们在实践中还是应该根据实际情况选择工具，工具没有好坏，适合的就是最好的。我们真正要做的是将业务逻辑抽象，做到尽量不依赖任何工具，换工具也最多只需要换一个适配器。 

和Redis一样，我们依然使用Docker启动服务。

```bash
docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant
```

同样也需要安装Python客户端。

```bash
$ pip install qdrant-client
```

安装好Python客户端后，就可以使用Python和Qdrant进行交互了。首先是生成Embedding，既可以使用OpenAI的get_embedding接口，也可以直接使用原生的Embedding.create接口，以支持批量请求。

```python
from openai.embeddings_utils import get_embedding, cosine_similarity
def get_embedding_direct(inputs: list):
    embed_model = "text-embedding-ada-002"
    res = openai.Embedding.create(
        input=inputs, engine=embed_model
    )
    return res
```

准备好数据后，批量获取Embedding。

```python
texts = [v.content for v in df.itertuples()]
len(texts) == 3964
import pnlp
emds = []
for idx, batch in enumerate(pnlp.generate_batches_by_size(texts, 200)):
    response = get_embedding_direct(batch)
    for v in response.data:
        emds.append(v.embedding)
    print(f"batch: {idx} done")
len(emds), len(emds[0]) == (3964, 1536)
```

generate_batches_by_size方法可以将一个可迭代的对象（此处是列表）拆成批量大小为200的多个批次。一次接口调用就可以获取200个文档的Embedding表示。 

接着是第2步，创建索引并入库。在此之前，先创建客户端，如下所示。

```python
from qdrant_client import QdrantClient
client = QdrantClient(host="localhost", port=6333)
```

值得一提的是，Qdrant还支持内存和文件库，也就是说，可以直接将Embedding放在内存或硬盘里。

```python
# client = QdrantClient(":memory:")
# 或
# client = QdrantClient(path="path/to/db")
```

创建索引的方法与Redis类似，只不过在Qdrant中是collection，如下所示。

```python
from qdrant_client.models import Distance, VectorParams
client.recreate_collection(
    collection_name="doc_qa",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
)
```

如果成功，则会返回True。可以使用下面的命令删除一个collection。

```python
client.delete_collection("doc_qa")
```

下面是向量入库代码。

```python
payload=[
    {"content": v.content, "heading": v.heading, "title": v.title, 
     "tokens": v.tokens} for v in df.itertuples()
]
client.upload_collection(
    collection_name="doc_qa",
    vectors=emds,
    payload=payload
)
```

接下来到第3步，检索相关文档。这里相比Redis简单很多，不需要构造复杂的查询语句。

```python
query = "Who won the 2020 Summer Olympics men's high jump?"
query_vector = get_embedding(query, engine="text-embedding-ada-002")
hits = client.search(
    collection_name="doc_qa",
    query_vector=query_vector,
    limit=5
)
```

我们获取到5个最相关的文档，第一个样例如下所示。

```python
ScoredPoint(id=236, version=3, score=0.90316474, payload={'content': 
'<CONTENT>', 'heading': 'Summary', 'title': 'Athletics at the 2020 Summer 
Olympics - Men’s high jump', 'tokens': 275}, vector=None)
```

由于篇幅受限，我们把content省略了，payload就是之前存进去的信息，我们可以在里面存储需要的任何信息。score是相似度得分，表示给定的query和向量库中存储的文档的相似度。 

接下来将这个过程和提示词的构建合并在一起。

```python
# 来自OpenAI官方示例
# 上下文的最大长度
MAX_SECTION_LEN = 500
# 当召回多个文档时，文档与文档之间的分隔符
SEPARATOR = "\n* "
separator_len = 3
def construct_prompt(question: str) -> str:
    query_vector = get_embedding(question, engine="text-embedding-ada-002")
    hits = client.search(
        collection_name="doc_qa",
        query_vector=query_vector,
        limit=5
    )
    choose = []
    length = 0
    indexes = []
    for hit in hits:
        doc = hit.payload
        length += doc["tokens"] + separator_len
        if length > MAX_SECTION_LEN:
            break
        choose.append(SEPARATOR + doc["content"].replace("\n", " "))
        indexes.append(doc["title"] + doc["heading"])
    # 简单的日志
    print(f"Selected {len(choose)} document sections:")
    print("\n".join(indexes))
    header = """Answer the question as truthfully as possible using the 
    provided context, and if the answer is not contained within the text below, 
    say "I don't know."\n\nContext:\n"""
    return header + "".join(choose) + "\n\n Q: " + question + "\n A:"
```

下面用一个例子验证一下。

```python
prompt = construct_prompt("Who won the 2020 Summer Olympics men's high 
jump?")
print("===\n", prompt)
"""
Selected 2 document sections:
Athletics at the 2020 Summer Olympics - Men's high jumpSummary
Athletics at the 2020 Summer Olympics - Men's long jumpSummary
===
Answer the question as truthfully as possible using the provided context, 
and if the answer is not contained within the text below, say "I don't 
know."
Context:
* <CONTENT 1>
* <CONTENT 2>
Q: Who won the 2020 Summer Olympics men's high jump?
A:
"""
```

限于篇幅，这里省略部分输出。对于找到的5个相关文档，由于上下文有长度限制（500），这里只使用前两个文档。 

构造好提示词后，最后一步就是基于给定文档回答问题。

```python
def complete(prompt: str) -> str:
    response = openai.Completion.create(
        prompt=prompt,
        temperature=0,
        max_tokens=300,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        model="text-davinci-003"
    )
    ans = response["choices"][0]["text"].strip("\n")
    return ans
``` 
