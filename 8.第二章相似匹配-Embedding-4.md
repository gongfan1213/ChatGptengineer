可以比较明显地看出，3种不同类型的数据分别位于3维空间中不同的位置。如果事先不知道每个样本有哪种标签（但我们知道标签类别有3个），则可以通过KMeans算法将数据划分为不同的族群，同一族群内相似度较高，不同族群之间相似度较低。KMeans算法的基本思路如下。
- 随机选择K（此处为3）个点作为初始聚类中心。
- 将每个点分配到距离最近的那个中心。
- 计算每个族群的平均值，将其作为新的聚类中心。
- 重复上述步骤，直到聚类中心不再明显变化为止。

示例代码如下。
```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3).fit(vis_dims)
```
然后就可以通过`kmeans.labels_`得到每个样本的标签了。

聚类任务在实际中直接用作最终方案的情况不是特别多（主要是因为无监督方法精度有限），而是用作辅助手段来对数据进行预划分，用于下一步的分析。但有一些特定场景比较适合使用这种方法，比如异常点分析、客户分群、社交网络分析等。

#### 2.3.3 推荐应用：一切都是Embedding

我们在很多App或网站上都能看到推荐功能。比如购物网站，每当你登录或选购一件商品后，系统就会给你推荐一些相关的商品。在本小节中，我们就来做一个类似的应用，不过我们推荐的不是商品，而是文本，比如帖子、文章、新闻等。我们以新闻为例，基本逻辑如下。

- 首先要有一个基础的文章库，其中可能包括标题、内容、标签等。

- 计算已有文章的Embedding并存储。

- 根据用户浏览记录，推荐和浏览记录最相似的文章。

这一次，我们使用Kaggle的AG_News数据集。上面的逻辑看起来好像和前面的QA任务的差不多，事实也的确如此，因为它们在本质上都是相似匹配问题。只不过QA任务使用用户的问题来匹配已有QA库中的问题，而推荐则使用用户的浏览记录来进行匹配。实际上，推荐相比QA任务要复杂一些，主要包括以下几个方面。

- 刚开始用户没有浏览记录时如何推荐的问题（一般被业界称为冷启动问题）。

- 除了相似还有其他要考虑的因素，比如热门内容、新内容、内容多样性、随时间变化的兴趣变化等。 

- 编码问题（Embedding的输入）：我们应该考虑标题还是文章，抑或考虑简要描述或摘要，或者考虑以上全部因素？ 


- 规模问题：推荐面临的量级一般会远超QA任务，除了横向扩展机器，是否能从流程和算法设计上提升效率？ 

- 用户反馈对推荐系统的影响问题：用户反感或喜欢与文章本身并没有直接关系，比如用户喜欢体育新闻但讨厌足球。 

- 线上实时更新问题。

当然，一个完整的线上系统要考虑的因素可能更多。列出这些只是希望读者在设计一个方案时能够充分调研和考虑，同时结合实际情况进行。反过来说，读者可能并不需要考虑上面的每个因素。所以，我们要活学活用，在实际操作时，要在充分理解需求后再动手实施。

我们在综合考虑了上面的因素后，得到一个比较简单的设计方案，但务必注意，其中每个模块的方案都不是唯一的。这个简单设计方案如下。

- 当用户注册并登录时，让其选择感兴趣的类型（如体育、音乐、时尚等），从而将用户限定在几个类别的范围内（推荐时可以只考虑这几个类别，提升效率），同时也可以用来解决冷启动问题。 

- 在给用户推荐内容时，根据用户注册时选择的类别或与用户浏览记录对应的类别，确定推荐类别后，接下来就应该依次考虑时效性、热门程度、多样性等。 


- 考虑性能问题，可以只编码标题和摘要。 

- 对大的类别做进一步细分，只在细分类别里进行相似度计算。 

- 记录用户实时行为，如浏览、评论、收藏、点赞、转发等。 

- 动态更新内容库，更新用户行为库。

现实场景中最常用的是流水线方案：召回 + 排序。召回就是通过各种不同属性或特征（如用户偏好、热点、行为等），先找到一批要推荐的列表。排序则指根据多样性、时效性、用户反馈、热门程度等属性对召回结果进行排序，将排在前面的优先推荐给用户。我们这里只简单展示召回。

这里使用Kaggle的AG_News数据集AG News Classification Dataset，该数据集可以从Kaggle官网搜索下载。还是老样子，先读取并查看数据。
```python
from dataclasses import dataclass
import pandas as pd

df = pd.read_csv("./dataset/AG_News.csv")
df.shape == (120000, 3)
df.head()
```
数据集中的前5条如表2-3所示，共包含3列——类别、标题和描述。

### 表2-3 AG_News数据集样例
|索引|类别|标题|描述|
| ---- | ---- | ---- | ---- |
|0|3|Wall St. Bears Claw Back Into the Black (Reuters)|Reuters - Short-sellers, Wall Street's dwindli...|
|1|3|Carlyle Looks Toward Commercial Aerospace (Reu...|Reuters - Private investment firm Carlyle Grou...|
|2|3|Oil and Economy Cloud Stocks' Outlook (Reuters)|Reuters - Soaring crude prices plus worries ab...|
|3|3|Iraq Halts Oil Exports from Main Southern Pipe...|Reuters - Authorities have halted oil export f...|
|4|3|Oil prices soar to all-time record, posing new...|AFP - Tearaway world oil prices, toppling reco...|

用`value_counts`查看类别统计。
```python
df["Class Index"].value_counts()
```
类别一共有4个，每个类别3万条数据。
```
3    30000
4    30000
2    30000
1    30000
Name: Class Index, dtype: int64
```
这4个类别分别是1-World（世界）、2-Sports（体育）、3-Business（商业）、4-Sci/Tech（科学与技术）。接下来，我们将使用前面介绍的知识做一个简单的流水线系统。为了便于展示，依然取100条数据作为示例。
```python
sdf = df.sample(100)
sdf["Class Index"].value_counts()
```
样本分布如下。
```
2    28
4    26
1    24
3    22
Name: Class Index, dtype: int64
```
首先需要维护用户偏好和行为记录，我们将相关的数据结构创建为dataclass类。
```python
from typing import List

@dataclass
class User:
    user_name: str

@dataclass
class UserPrefer:
    user_name: str
    prefers: List[int]

@dataclass
class Item:
    item_id: str
    item_props: dict

@dataclass
class Action:
    action_type: str
    action_props: dict

@dataclass
class UserAction:
    user: User
    item: Item
    action: Action
    action_time: str
```
创建几条数据以便后面演示。
```python
u1 = User("u1")
up1 = UserPrefer("u1", [1, 2])
i1 = Item("i1", {
    "id": 1,
    "category": "sport",
    "title": "Swimming: Shibata Joins Japanese Gold Rush",
    "description": "ATHENS (Reuters) - Ai Shibata wore down French teen-ager Laure Manaudou to win the women's 800 meters freestyle gold medal at the Athens Olympics Friday and provide Japan with their first female swimming champion in 12 years.",
    "content": "content"
})
a1 = Action("浏览", {
    "open_time": "2023-04-01 12:00:00",
    "leave_time": "2023-04-01 14:00:00",
    "type": "close",
    "duration": "2hour"
})
ua1 = UserAction(u1, i1, a1, "2023-04-01 12:00:00")
```
接下来计算所有文本的Embedding，这一步和之前一样。
```python
from openai.embeddings_utils import get_embedding, cosine_similarity
from sklearn.metrics.pairwise import cosine_similarity
import openai
import numpy as np

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY

sdf["embedding"] = sdf.apply(
    lambda x: get_embedding(x.Title + x.Description, engine="text-embedding-ada-002"), axis=1
)
```
这里为简单起见，我们直接对标题Title和描述Description进行了拼接。

召回模块涉及以下三种不同的召回方式。
- 根据用户行为记录召回。首先获取用户行为记录，一般在数据库中查表可得。我们忽略数据库操作，直接固定输出。然后获取用户最感兴趣的条目，可以选近一段时间内用户浏览时间长、次数多，收藏、评论过的条目。最后根据用户感兴趣的条目展开推荐，这里和之前的QA任务一样——根据用户感兴趣的条目，在同类别下找到相似度最高的条目。 
- 根据用户偏好召回。这种召回方式比较简单，就是在用户偏好的类别下随机选择一些条目，这种召回方式往往用在冷启动问题上。 
- 热门召回。真实场景下肯定是有一个动态列表的，我们这里就随机选择了。
```python
import random

class Recall:
    """
    召回模块，代码比较简单，只是为了展示流程
    """
    def __init__(self, df: pd.DataFrame):
        self.data = df

    def user_prefer_recall(self, user, n):
        up = self.get_user_prefers(user)
        idx = random.randrange(0, len(up.prefers))
        return self.pick_by_idx(idx, n)

    def hot_recall(self, n):
        # 随机选择示例
        df = self.data.sample(n)
        return df

    def user_action_recall(self, user, n):
        actions = self.get_user_actions(user)
        interest = self.get_most_interested_item(actions)
        recoms = self.recommend_by_interest(interest, n)
        return recoms

    def get_most_interested_item(self, user_action):
        idx = user_action.item.item_props["id"]
        im = self.data.iloc[idx]
        return im

    def recommend_by_interest(self, interest, n):
        cate_id = interest["Class Index"]
        q_emb = interest["embedding"]
        # 确定类别
        base = self.data[self.data["Class Index"] == cate_id]
        # 用给定embedding计算base中embedding的相似度
        base_arr = np.array([v.embedding for v in base.itertuples()])
        q_arr = np.expand_dims(q_emb, 0)
        sims = cosine_similarity(base_arr, q_arr)
        # 排除自身
        idxes = sims.argsort()[0].squeeze()[:-(n+1):-1]
        return base.iloc[reversed(idxes.tolist())]

    def pick_by_idx(self, category, n):
        df = self.data[self.data["Class Index"] == category]
        return df.sample(n)

    def get_user_actions(self, user):
        dct = {"u1": ua1}
        return dct[user.user_name]

    def get_user_prefers(self, user):
        dct = {"u1": up1}
        return dct[user.user_name]

    def run(self, user):
        ur = self.user_action_recall(user, 5)
        if len(ur) == 0:
            ur = self.user_prefer_recall(user, 5)
        hr = self.hot_recall(3)
        return pd.concat([ur, hr], axis=0)
```
执行一下，看看效果如何。
```python
r = Recall(sdf)
rd = r.run(u1)
```
我们得到8个条目，其中5个是根据用户行为推荐的，剩下的3个是热门推荐，如表2-4所示。

一个简单的推荐系统就做好了。需要再次说明的是，这只是一个大致的流程。 

程，而且只有召回。在实际场景中，上面的每个地方都需要优化。我们简单罗列一些优化点供读者参考。

- 建数据库表（前面以get_开头的方法实际上都是在查表），并处理增、删、改逻辑。

- 将用户、行为记录等也Embedding化。这与文本无关，但确实是真实场景中的方案。 

- 对“感兴趣”模块进行更多的优化，考虑更多用户行为和反馈，召回更多不同类别的条目。 

- 考虑性能和自动更新数据方面。 

- 进行线上评测、A/B测试等。 

### 表2-4 推荐结果列表


|索引|类别|标题|描述|
| ---- | ---- | ---- | ---- |
|12 120|2|Olympics Wrap: Another Doping Controversy Surf...|ATHENS (Reuters) - Olympic chiefs ordered Hun...|
|5 905|2|Saturday Night #39;s Alright for Blighty|Matthew Pinsents coxless four team, sailor Ben...|
|29 729|2|Beijing Paralympic Games to be fabulous: IPC P...|The 13th Summer Paralympic Games in 2008 in Be...|
|27 215|2|Dent tops Luczak to win in China Open|Taylor Dent defeated Australian qualifier Pete...|
|72 985|2|Rusedski through in St Petersburg|Greg Rusedski eased into the second round of t...|
|28 344|3|Delta pilots wary of retirements|Union says pilots may retire en masse to get p...|
|80 374|2|Everett powerless in loss to Prince George|Besides the final score, there is only one sta...|
|64 648|4|New Screening Technology Is Nigh|Machines built to find weapons hidden in cloth...|

可以发现，我们虽然只做了召回，但其中涉及的内容已经远不止之前QA任务那一点内容了，QA任务用到的知识在这里可能只是其中的少部分。不过事无绝对，即便是QA任务，也可能需要根据实际情况做很多优化。但总体来说，类似推荐这样比较综合的系统相对来说会更加复杂一些。

后面的排序模块需要区分不同的应用场景，可以做或不做。做也可以简单做或复杂做，比如简单做就按发布时间，复杂做就要综合考虑多样性、时效性、用户反馈、热门程度等多种因素。具体操作时，可以直接按相关属性排序，也可以用模型排序。受限于主题和篇幅，我们不再探讨。

### 2.4 本章小结
相似匹配是整个人工智能算法领域非常基础和重要的任务，NLP、搜索、图像、推荐等方向都涉及相似匹配，而Embedding就是其中最重要的一项技术。通俗点来说，就是把数据表示成空间中的一个点，并通过稠密向量来表示复杂语义信息。Embedding化之后，不同类型的数据就可以彼此交互、融合，从而得到更好的效果。即便强大如ChatGPT这样的大语言模型，也依然需要Embedding表示技术来更好地获取上下文。随着多模态技术的不断发展，Embedding在未来可能会变得更加重要。 
