### 第 1 章 基础知识——大语言模型背后
本章共包括三部分内容。首先简要回顾自然语言发展历史，从语言、图灵测试一直到 2022 年年底的新突破——ChatGPT；接下来介绍语言模型基础，包括 Token、Embedding 等基本概念和语言模型的基本原理，它们是自然语言处理（natural language processing, NLP）最基础的知识；最后介绍与 ChatGPT 相关的基础知识，包括 Transformer、GPT 和 RLHF。Transformer 是 ChatGPT 的基石，准确来说，Transformer 的一部分是 ChatGPT 的基石；GPT（generative pre-trained transformer，生成式预训练 Transformer）是 ChatGPT 的本体，从 GPT-1 到现在的 GPT-4，按 OpenAI 的说法，模型还是那个模型，只是规模更大、效果更好了；RLHF（reinforcement learning from human feedback，从人类反馈中强化学习）是 ChatGPT 的神兵利器，有此利刃，ChatGPT 所向披靡。

#### 1.1 自然语言背景
##### 1.1.1 语言是智能的标志
很久以前，有个神奇星球，上面生物多样，却因交流方式独特无法有效沟通。一天，神秘外星人到来，决定赋予一种生物全新沟通方式——“语言”，为此举办比赛。最终人类脱颖而出，迅速掌握语言知识，还不断创新词汇和表达方式。外星人宣布人类获胜并赋予语言能力，从此人类利用语言建立复杂社会体系、发展科技、创作艺术，在星球上独树一帜。

这个故事虽非真实发生，但客观上语言确实为人类独有。在大自然进化中，各物种有独特技能，而人类是唯一能调控呼气声音进行信息交流、描述事件的灵长类动物。语言成为人类发展和创新的强大工具，在社交、科研、艺术等方面至关重要，是人类区别于其他物种的标志性特征，也成为判断机器是否具备类人智能的重要标准。

##### 1.1.2 从图灵测试到 ChatGPT
1950 年，图灵发表论文“Computing Machinery and Intelligence”，尝试回答“机器能否思考”问题，提出“图灵测试”（即模仿游戏）概念检测机器智能水平。其核心是，测试者 C 用语言询问两个看不见的对象 A（机器）和 B（正常思维的人），若 C 无法分辨 A 与 B 的不同，则机器 A 通过图灵测试。

1956 年，人工智能成为科学概念，此后相关研究不断涌现。图灵测试是启发性思想实验，非具体执行的判断方法，却阐明了“智能”判断的模糊性与主观性，成为 NLP 任务重要评测标准，提供了客观直观评估机器智能的方式，避免了智能本质的哲学争论，绕开智能具体表现形式的技术细节。许多 NLP 任务如聊天机器人、问答系统、文本生成等都可用图灵测试评测。

NLP 是计算机科学、人工智能和语言学的交叉领域，关注计算机和人类语言的相互作用，常见任务和应用包括信息抽取、文本分类、文本摘要、机器翻译、问答系统、聊天机器人等。图灵测试与 NLP 任务关系密切且复杂，体现在：
 - 图灵测试是 NLP 任务的重要驱动力，提出让机器用自然语言与人类流畅、智能、多样化对话的目标，促使 NLP 领域不断发展创新技术方法，提高机器对自然语言的理解和生成能力，如研究问答系统、文本生成等任务。
 - 图灵测试是 NLP 任务的重要目标，激励 NLP 领域研究者和开发者探索创新，实现机器达到与人类相同甚至超越人类的智能水平，如研究语义分析、知识表示、逻辑推理等。

NLP 与人工智能发展史相互促进、影响、依存、互为目标，有许多里程碑成果：
 - 1954 年，IBM 实现世界首个机器翻译系统，将俄语译成英语。
 - 1966 年，Joseph Weizenbaum 开发 ELIZA，一种模拟心理治疗师的聊天机器人。
 - 1972 年，Terry Winograd 开发 SHRDLU，能理解和生成自然语言，用于控制虚拟机器人在虚拟世界操作。
 - 2011 年，苹果推出 Siri，基于 NLP 技术的智能语音助手；同年，IBM 的 Watson 战胜《危险边缘》节目冠军选手，展示 NLP 技术在问答领域的强大能力。
 - 2013 年，谷歌推出 Word2Vec，基于神经网络的词向量表示方法，开启 NLP 深度学习时代。
 - 2016 年，Facebook（现 Meta）发布 fastText，用于文本分类，在处理大规模文本分类任务时效果好。
 - 2017 年，谷歌发布论文“Attention is All You Need”，提出 Transformer，具有多头注意力机制，在文本特征提取方面效果优异。
 - 2018 年，谷歌发布 BERT（bidirectional encoder representations from transformers，基于 Transformer 的双向编码器表示）预训练模型，在多项 NLP 任务中取得最佳效果。
 - 2020 年，OpenAI 发布的 GPT-3 模型有多达 1750 亿个参数，能在少量样本或无样本前提下完成多数 NLP 任务。

2022 年 11 月 30 日，OpenAI 发布 ChatGPT，一经发布便点燃人工智能圈，5 天用户量达 100 万，OpenAI 紧急扩容。用户发现 ChatGPT 不仅能自然流畅聊天，还能写论文、讲笑话、编段子、生成演讲稿、写请假条、模仿导师写推荐信，甚至写代码、写营销策划案等。


到 2023 年 1 月，ChatGPT 成为史上用户量最快达 1 亿的应用。无论是 ChatGPT 还是其他大语言模型，使用时部分参数可能需调整。对业内人员来说通常问题不大，但非业内人员可能觉得专业。本章后续主要介绍 ChatGPT 相关技术基本原理，尽量用浅显语言表述，助读者了解原理以更好使用 ChatGPT。

#### 1.2 语言模型基础
##### 1.2.1 最小语义单位 Token 与 Embedding
将自然语言文本表示成计算机能识别的数字，需先把文本变成一个个 Token。Token 可理解为小块，比如英文中单词 annoyingly 可拆成 ["annoying", "##ly"] ，“##” 表示与前一个 Token 直接拼接无空格；中文基本用字 + 词方式，字能独立表意如 “是”“有”“爱”，词由多个字组成，拆开可能丢失语义如 “长城”“情比金坚” 。获取 Token 可分词或分字，英文多用子词，中文根据任务类型和效果选择拆分方式。

当句子表示成 Token 后，可用数字表示。简单方法是每个 Token 用一个数字表示，但这种方式表达单调，不能表示丰富语言信息。词袋模型（bag of words, BOW）将文本中每个词看作独立的，忽略词序和语法，只关注词出现次数，每个文本表示为向量，向量维度对应词，值为词在文本中出现次数。但词袋模型存在问题：一是词表大导致向量维度高且稀疏，计算不便；二是忽略 Token 顺序，部分语义丢失，如 “你爱我” 和 “我爱你” 向量表示相同但意思不同。

于是词向量（词嵌入）出现，它是稠密表示方法，一个 Token 表示成一定数量小数，词向量维度常见值有 200、300、768、1536 等。早期词向量静态，训练完固定不变，随 NLP 技术发展，演变成基于语言模型的动态表示，上下文不同时，同一词向量表示不同，且句子表示在模型架构设计上做了考虑，输入句子可直接获得句子向量。将任意文本（或其他非文本符号）表示成稠密向量的方法统称 Embedding 表示技术，它是 NLP 领域基础技术，深度学习模型多基于此，甚至可说深度学习发展就是 Embedding 表示技术的发展。

##### 1.2.2 语言模型是怎么回事
语言模型（language model, LM）是利用自然语言构建的模型，根据给定文本输出对应文本。概率语言模型核心是概率，通过已有 Token 预测接下来的 Token 。例如只告诉模型 “我喜欢你” 这句话，输入 “我” 时，模型可能预测下一个是 “喜欢” 。若给模型大量资料，输入 “我” 时，它可能不会只说 “喜欢” 。

若每次只选概率最大的 Token ，这种方法叫贪心搜索（greedy search），模型表现较 “呆” 。为使生成结果多样丰富，语言模型会采用一些策略，如集束搜索（beam search），即一步多看几个词，看最终句子（到句号、感叹号等停止符号）的概率。例如，从 “我” 开始，考虑 “喜欢” 和 “想” ，再看下一步概率，“我喜欢你” 概率为 0.3×0.8 = 0.24 ，“我喜欢吃” 概率为 0.3×0.1 = 0.03 ，“我想你” 概率为 0.4×0.5 = 0.2 ，“我想去” 概率为 0.4×0.3 = 0.12 ，最终概率最大的是 “我喜欢你”  。集束搜索中 num_beams 表示同时考虑的词数，看得越多越不易生成固定文本。

早期语言模型主要研究模型本身，经历从简单到复杂再到巨大复杂模型的变迁。简单模型如 N - Gram 模型，把句子拆成 Token 统计概率，N 表示每次用到的上文 Token 个数，N 通常为 2 或 3 ，等于 2 叫 Bi - Gram ，等于 3 叫 Tri - Gram 。例如句子 “人工智能让世界变得更美好” ，Bi - Gram 为：人工智能/让 让/世界 世界/变得 变得/更 更/美好 ；Tri - Gram 为：人工智能/让/世界 让/世界/变得 世界/变得/更 变得/更/美好 。Bi - Gram 下一个 Token 根据上一个 Token 而来，Tri - Gram 下一个 Token 根据前两个 Token 而来 。 

多看一步大不一样！看看概率最大的成谁了，变成了“我喜欢你”。上面这种方法叫作集束搜索（beam search），简单来说，就是一步多看几个词，看最终句子（比如生成到句号、感叹号或其他停止符号）的概率。在上面的例子中，num_beams=2（只看了两个词），看得越多，越不容易生成固定的文本。 

好了，其实在最开始的语言模型中，基本就到这里，上面介绍的两种不同搜索方法（贪心搜索和集束搜索）也叫解码策略。当时更多被研究的还是模型本身，我们经历了从简单模型到复杂模型，再到巨大复杂模型的变迁过程。简单模型就是把一句话拆成一个个Token，然后统计概率，这类模型有个典型代表——N-Gram模型，它也是最简单的语言模型。这里的N表示每次用到的上文Token的个数。举个例子，看下面这句话：“人工智能让世界变得更美好”。N-Gram模型中的N通常等于2或3，等于2的叫Bi-Gram，等于3的叫Tri-Gram。 
 - Bi-Gram：人工智能/让 让/世界 世界/变得 变得/更 更/美好 
 - Tri-Gram：人工智能/让/世界 让/世界/变得 世界/变得/更 变得/更/美好 

Bi-Gram和Tri-Gram的区别是，前者的下一个Token是根据上一个Token来的，而后者的下一个Token是根据前两个Token来的。在N-Gram模型中，Token的表示是离散的，实际上就是词表中的一个个单词。这种表示方式比较简单，再加上N不能太大，导致难以学到丰富的上下文知识。事实上，它并没有用到深度学习和神经网络，只是一些统计出来的概率值。以Bi-Gram为例，在给定很多语料的情况下，统计的是从“人工智能”开始，下个词出现的频率。假设“人工智能/让”出现了5次，“人工智能/是”出现了3次，将它们出现的频率除以所有的Gram数就是概率。 

训练N-Gram模型的过程其实是统计频率的过程。如果给定“人工智能”，N-Gram模型就会找基于“人工智能”下个最大概率的词，然后输出“人工智能 让”。接下来就是给定“让”，继续往下走了。当然，我们也可以用上面提到的不同解码策略往下走。 

接下来，让每个Token成为一个Embedding向量。我们简单解释一下在这种情况下怎么预测下一个Token。其实还是计算概率，但这次和刚才的稍微有点不一样。在刚才离散的情况下，用统计出来的对应Gram数除以Gram总数就是出现概率。但是稠密向量要稍微换个方式，也就是说，给你一个d维的向量（某个给定的Token），你最后要输出一个长度为N的向量，N是词表大小，其中的每一个值都是一个概率值，表示下一个Token出现的概率，概率值加起来为1。按照贪心搜索解码策略，下一个Token就是概率最大的那个，写成简单的计算表达式如下。 
```
# d维，加起来和1没关系，大小是1×d，表示给定的Token
X = [0.001, 0.002, 0.0052, ..., 0.0341] 
# N个，加起来为1，大小是1×N，表示下一个Token就是每个Token出现的概率
Y = [0.1, 0.5, ..., 0.005, 0.3] 
# W是模型参数，也可以叫模型
X·W = Y # W可以是d×N大小的矩阵
```
上面的W就是模型参数，其实X也可以被看作模型参数（自动学习到的）。因为我们知道了输入和输出的大小，所以中间其实可以经过任意的计算，也就是说，W可以包含很多运算。总之各种张量（三维以上数组）运算，只要保证最后的输出形式不变就行。各种不同的计算方式就意味着各种不同的模型。 

在深度学习早期，最著名的语言模型是使用循环神经网络（recurrent neural network, RNN）训练的，RNN是一种比N-Gram模型复杂得多的模型。RNN与其他神经网络的不同之处在于，RNN的节点之间存在循环连接，这使得它能记住之前的信息，并将它们应用于当前的输入。这种记忆能力使得RNN在处理时间序列数据时特别有用，例如预测未来的时间序列数据、进行自然语言的处理等。通俗地说，RNN就像具有记忆能力的人，它可以根据之前的经验和知识对当前的情况做出反应，并预测未来的发展趋势，如图1-4所示。 

![image](https://github.com/user-attachments/assets/739d391d-7fd7-4a97-89cf-b0dabc0ce5d3)


**图1-4 RNN（摘自Colah的博客文章“Understanding LSTM Networks”）**
（此处为RNN示意图，未详细描述图中内容）

在图1-4中，右边是左边的展开，A就是参数，x是输入，h就是输出。自然语言是一个Token接着一个Token（Token by Token）的，从而形成一个序列。参数怎么学习呢？这就要稍微解释一下学习（训练）过程。 

如图1-5所示，第一行就是输入X，第二行就是输出Y，SOS（start of sentence）表示句子开始，EOS（end of sentence）表示句子结束。注意，图1-4中的h并不是那个输出的概率，而是隐向量。如果需要概率，可以再对h执行张量运算，归一化到整个词表即可。 

**图1-5 语言模型学习（训练）时的输入输出**
（此处为输入输出示意图，未详细描述图中内容）

![image](https://github.com/user-attachments/assets/28060e7d-cc71-4077-8388-7449e8e59ee6)


```python
import torch
import torch.nn as nn

rnn = nn.RNN(32, 64)
input = torch.randn(4, 32)
h0 = torch.randn(1, 64)
output, hn = rnn(input, h0)
output.shape, hn.shape
# (torch.Size([4, 64]), torch.Size([1, 64]))
```
上面的nn.RNN就是RNN模型。输入是一个4×32的向量，换句话说，输入是4个Token，维度d=32。h0就是随机初始化的输出，也就是4个Token中第1个Token的输出，这里output的4个64维的向量分别表示4个输出。hn就是最后一个Token的输出（它和output的最后一个64维向量是一样的），也可以看成整个句子的表示。注意，这里的output和图1-5中的输出Y还没有关系。别急，继续往下看。如果要输出词的概率，就需要先扩充到词表大小，再进行归一化。 
```python
# 假设词表大小N=1000
wo = torch.randn(64, 1000)
# 得到4×1000的概率矩阵，每一行概率和为1
probs = nn.Softmax(dim=1)(output @ wo)
probs.shape, probs.sum(dim=1)
# torch.Size([4, 1000]), tensor([1.0000, 1.0000, 1.0000, 1.0000],
# grad_fn=<SumBackward1>)
```
这里的probs的每一行就是词表大小的概率分布，概率和为1，意思是根据当前Token生成下一个Token的概率，下一个Token有可能是词表中的任意一个Token，但它们的概率和一定为1。因为我们知道接下来每个位置的Token是什么（也就是图1-5中的输出Y）。这里得到最大概率的那个Token，如果正好是这个Token，则说明预测对了，参数就不用怎么调整；反之，模型就会调整前面的参数（RNN、h0、input和wo）。你可能会疑惑为什么input也是参数，其实前面我们偷懒了，本来的参数是一个1000×32的大矩阵，但我们使用了4个Token对应位置的向量。这个1000×32的大矩阵其实就是词向量（每个词一行），开始时全部随机初始化，然后通过训练调整参数。 

训练完成后，这些参数就不变了，然后就可以用前面同样的步骤来预测了，也就是给定一个Token，预测下一个Token。如果使用贪心搜索，则每次给定同样的Token时，生成的结果就一样。其余的就和前面讲的接上了。随着深度学习的不断发展，出现了更多比RNN还复杂的网络结构，而且模型变得更大，参数更多，但逻辑和方法是一样的。 

好了，语言模型就介绍到这里。上面的代码看不懂没关系，你只需要大致了解每个Token是怎么表示、怎么训练和预测出来的就行。简单直观地说，构建（训练）语言模型的过程就是学习词、句内在的“语言关系”；而推理（预测）就是在给定上下文后，让构建好的模型根据不同的解码策略输出对应的文本。无论是训练还是预测，都以Token为粒度进行。 

 
