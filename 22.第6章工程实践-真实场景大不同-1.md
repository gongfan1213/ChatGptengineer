### 第6章 工程实践——真实场景大不同
通过前面章节的学习，相信读者应该已经具备一定的NLP算法应用开发能力。虽然需要借助大语言模型，但这也是一种能力，毕竟用户并不关心产品背后用了什么技术。一款产品或应用开发完成后，接下来就要面对市场和客户了，这中间有非常多的工作要做。即便我们只是为整个产品或服务提供一个接口，那也有许多要考虑的因素。

在第2章和第3章中，我们在应用部分提到了一些开发应用时需要注意的事项，不过它们大多和具体的应用相关。在本章中，我们将从整体上介绍利用大语言模型接口开发一款要上线面向市场的应用或服务所应该考虑的内容。我们将重点关注三个方面：首先是评测，它是一款应用或服务能否上线的标准；然后是安全，它是上线一款应用或服务所不得不考虑的话题；最后是网络，其中涉及一些网络请求方面的设计和技巧。

#### 6.1 评测：决定是否上线的标准
##### 6.1.1 为什么评测
在之前的章节中，除第3章中的微调部分外，几乎没有涉及这个话题，但其实这非常重要。首先，我们来看为什么需要评测。在工程开发中，测试工程师的主要职责就是对产品的各个功能进行各种各样的测试，以保证产品功能正常、符合预期。我们在服务上线前，往往也会对自己的接口进行压测（即压力测试），看看能否达到上线标准。

对于一个算法模型来说，这一步是类似的，只不过所要评测的是模型输出的内容是否符合我们的预期目标。从理论上讲，我们永远都能找到模型预测错误的负例，所以实际上线后的结果不可能百分之百正确。这是开发算法模型与开发其他功能不太一样的地方，因为它在本质上是一个根据已有数据学习到一种策略，然后预测新数据的过程。

测试往往需要一批数据，这批数据应该尽量和真实场景接近，但它们一定不能包含在训练数据中。评测一个模型其实就是评测这个模型在它“未见过”样例上的效果。在实际场景中，我们往往会从整个数据集中分出去一部分数据作为测试集，它的分布与训练集是一致的。测试数据一般占整个数据集的20%，但这不是必需的，可以视具体情况增大或减小比例。对于一个语言模型来说，比较重要的一点是，构造模型输入和长度截断的方法应和训练集一致。通过模型得到输出后，我们需要对输出和标准答案进行对比，然后统计相关数据，最终得到评测指标。

##### 6.1.2 NLU常用评测指标

不同的任务往往采用不同的评测指标，对于NLU任务来说，我们一般使用精准率（P）、召回率（R）、F1值等指标，它们一般可以通过表6 - 1所示的混淆矩阵计算得到。

|真实情况|预测结果正例|预测结果负例|
| ---- | ---- | ---- |
|正例|真正例（true positive, TP）|假负例（false negative, FN）|
|负例|假正例（false positive, FP）|真负例（true negative, TN）|
具体的计算方法如式（6.1）~式（6.3）所示。
\[P = \frac{TP}{TP + FP}\]（6.1）
\[R = \frac{TP}{TP + FN}\]（6.2）
\[F_1 = \frac{2 \times P \times R}{P + R}\]（6.3）

通常情况下，精准率和召回率是一种权衡关系，提高精准率就会降低召回率，反过来也一样。F1值则综合考虑了它们两者。举个例子，假设我们有一个垃圾邮件分类器，测试集中的邮件数量为100，其中垃圾邮件（此处为正例）为20封，其余80封为正常邮件。模型预测结果为30封邮件是正例，但其实只有15封邮件是真正的垃圾邮件。也就是说，这30封邮件中有15封被正确识别为垃圾邮件，另外15封其实是正常邮件，它们被误识别为垃圾邮件；同时，模型预测结果为70封邮件是负例，其中有5封其实是垃圾邮件，未被模型正确识别。此时，混淆矩阵如表6 - 2所示。
|真实情况|预测结果正例 = 30|预测结果负例 = 70|
| ---- | ---- | ---- |
|正例 = 20|TP = 15|FN = 5|
|负例 = 80|FP = 15|TN = 65|
对应的指标如式（6.4）~式（6.6）所示。
\[P = \frac{15}{15 + 15} = 0.5\]（6.4）
\[R = \frac{15}{15 + 5} = 0.75\]（6.5）
\[F_1 = \frac{2 \times 0.5 \times 0.75}{0.5 + 0.75} = 0.6\]（6.6）

![image](https://github.com/user-attachments/assets/49a06306-8f6a-4b25-9b17-fb9f0e25d80c)

在这个例子中，应该找到20封垃圾邮件，模型找到了15封，召回率就是15/20；但是模型一共预测出了30封垃圾邮件，精准率只有15/30。在这个场景下，没有识别出来垃圾邮件是可以接受的，但如果把用户的正常邮件识别为垃圾邮件，那用户肯定会有意见。所以，我们要保证精准率很高才行，换句话说，要做到被标记为垃圾邮件的一定是垃圾邮件（虽然可能会漏掉一些垃圾邮件）。此时要求的概率阈值就比较高，比如95%，只有大于该阈值才判定为垃圾邮件。这也就意味着，本来模型判定为垃圾邮件的，有可能因为概率没有达到95%而被标记为正常邮件。虽然精准率非常高，但同时也会漏掉很多垃圾邮件，导致召回率下降。反过来，如果召回率上升，则意味着更多正常邮件可能被识别为垃圾邮件，导致精准率下降。调整后的混淆矩阵如表6 - 3所示，读者不妨自己重新计算这个指标。

|真实情况|预测结果正例 = 10|预测结果负例 = 90|
| ---- | ---- | ---- |
|正例 = 20|TP = 10|FN = 10|
|负例 = 80|FP = 0|TN = 80|

当然，如果模型能够让这些被识别为负例的正例的概率提高到阈值之上，换句话说，模型效果更好了（更加肯定判定的垃圾邮件就是垃圾邮件），那么精准率和召回率就会同时提升，F1值也会跟着提升。如果能达到这种理想情况，那肯定是最好的，但实践中往往需要权衡，毕竟有些样本实在太难以辨认了。

如果是多分类，则需要分别计算每一个类别的指标，然后加以综合，综合方法有两种。
- macro（宏）方法：先计算每个类别的精准率和召回率，取平均后，再计算F1值。
- micro（微）方法：先计算混淆矩阵中元素的平均值，再计算精准率、召回率和F1值。

当各个类别的重要性相对平衡时，可以使用macro方法；当更关心总体性能而非每个类别的性能时，可以使用micro方法。以类别样本不均衡的情况为例，如果想要平等地看待每个样本，也就是无论类别样本是否均衡，所有样本都一视同仁，则可以选择micro方法；但是，如果觉得类别是平等的，样本多的和样本少的类别应该平等看待，则可以选择macro方法。

##### 6.1.3 NLG常用评测指标
在NLU任务中，无论是句子分类、Token分类还是其他匹配问题，一般会有一个标准答案。但NLG任务不太一样，因为是生成式的，所以很难保证输出的内容和标准答案一模一样，更不用说，很多任务根本没有标准答案。生成式文本摘要、翻译等任务往往有参考答案，我们至少还有比对的标准；但像生成式写作、自由问答、对话这种，很多时候就需要针对性地设计评测指标。下面我们针对有参考答案和没有参考答案的任务分别举一个和大语言模型相关的例子来进行说明。

对于有参考答案的任务，我们以生成式文本摘要为例，它要求模型在给定一段文本后，能用几句话概述这段文本的主要内容和思想，具体可以参考第4章。因为这种任务有参考答案，所以我们经常采用相似度来衡量，即计算生成的内容与参考答案之间的相似度。这种做法和我们之前在第2章中的做法一样。或者，也可以单独训练一个二分类模型，判断生成的内容和参考答案是否相似。更进一步地，我们可以细化到Token粒度，从语义角度进行评估，常用的方法是BERTScore；或者从字面量角度进行评估，常用的方法是BLEU和ROUGE。

BERTScore借助BERT这样的预训练模型计算Token的Embedding，然后计算所生成内容的Token和参考答案Token之间的相似度，并进一步根据式（6.7）和式（6.8）计算精准率和召回率。
\[P = \frac{1}{|\hat{x}|}\sum_{\hat{x}_j}\max_{x_i \in x}\text{SimArray}\]（6.7）
\[R = \frac{1}{|x|}\sum_{x_i}\max_{\hat{x}_j \in \hat{x}}\text{SimArray}\]（6.8）

![image](https://github.com/user-attachments/assets/999e4365-f104-42eb-b17b-c6b2bb52f55b)


其中，x是参考答案的Token，\(\hat{x}\)是所生成内容的Token。有了精准率和召回率，就可以进一步根据式（6.3）计算得到F1值。我们举个具体的例子，如下所示。

```python
ref = "我爱伟大祖国"
hyp = "我爱祖国"
# emd_r shape => (1, 6, 768)
# emd_h shape => (1, 4, 768)
# emd_h、emd_r均为PyTorch的张量
sim = emd_h @ emd_r.transpose(1, 2)
# sim shape => (1, 4, 6)
sim = array([
    [0.9830,0.5148,0.5112,0.5310,0.4482,0.4427],
    [0.4854,0.9666,0.9402,0.5899,0.8704,0.3159],
    [0.4613,0.8755,0.9184,0.5819,0.9397,0.3576],
    [0.4456,0.3135,0.3572,0.5036,0.3696,0.9722]
])
```

给定参考答案为ref，生成的内容为hyp。通过BERT等预训练模型，首先得到每个Token的向量表示（其中，1为批量大小、768为向量维度、6和4为Token数），然后通过矩阵乘法得到相似度数组sim。接下来，根据式（6.7）和式（6.8）分别计算精准率和召回率，如下所示。

```python
p = 1/4 * sim.max(axis=2).sum()
r = 1/6 * sim.max(axis=1).sum()
```

根据得到的精准率和召回率，进一步计算F1值。可以看出，精准率是\(\hat{x}\)中的每个Token匹配到x中的一个Token，即匹配到0.9830、0.9666、0.9397和0.9722，对应的位置在x中的Token是“我”“爱”“祖”“国”；而召回率是x中的每个Token匹配到\(\hat{x}\)中的一个Token，“伟大”两个字没有匹配到，但“伟”字的分数还可以（0.9402），“大”的分数就只有0.5899了。总的来说，这是一种从Token的语义角度来对比生成内容和参考答案的评测方法。

和BERTScore不同的是，BLEU和ROUGE是按照字面量是否完全相同来进行比较的，这里的字面量通常选择N - Gram，N一般选择多个同时使用（比如1、2、3、4）。它们的算法有不少细节，但如果从简单直观的角度理解，前者衡量有多少个生成内容的Gram出现在参考答案中，后者衡量有多少个参考答案的Gram出现在生成内容中。前者类似于精准率，后者则类似于召回率，它们也可以使用式（6.3）计算得到F1值。不过单独使用也没问题，这一般取决于具体的任务，比如文本摘要就常用ROUGE，因为相对而言，我们更加关注生成的摘要有没有概括给定文本，即有没有覆盖参考答案。而翻译任务则常用BLEU，因为我们更加关注翻译出来的内容是否正确。

以上评测方法都有很多现成的工具包，安装后即可方便地使用，读者在实际使用它们时可自行通过搜索引擎来搜索和安装。

对于没有参考答案的任务，我们以文案生成为例，这是非常适合大语言模型的一个任务。具体来说，就是给定大量商品和用户属性，让模型针对用户就给定商品生成一段有说服力的销售文案或推荐话术。此类任务一般需要专人进行评估，当然，我们也可以让更好的大语言模型充当人工角色，我们要做的就是设计评测指标和标准。标准一般视需求而定，但要重点考虑以下因素。

- **准确性**：生成的内容是否包含关键必要信息（比如商品名称、价格等），这些信息是否有误。


- **流畅性**：生成的内容读起来是否通顺、合理、有逻辑。这里的流畅不仅指字面上的流畅，还包括语义上的流畅。 

- **生动性**：生成的内容是否有吸引力，能够让用户产生购买欲。这个要求有点高，但也并非完全做不到。

我们可以针对上面的每一项进行打分，既可以采用多人打分取平均；也可以对多个模型或服务商进行评测，选择其中最合适的。需要再次强调的是，指标的设计务必结合实际场景，综合权衡成本和效果，不必追求非常全面。

#### 6.2 安全：必须认真对待的话题
安全是指模型生成的内容不应该包含任何偏见、敏感内容、风险等。这是产品的生命线，安全不过关，产品必然被下线。所以，作为服务提供方，如果使用了生成式语言模型，那么当产品在效果上被验证可以达到要求后，接下来需要重点关注的就是安全。

##### 6.2.1 前/后处理
前处理和后处理是我们所能想到的最直观的解决方案。前处理是指在将用户的输入传递给模型之前，必须进行一次风险检查，这里可以是一个模块或外部接口（国内很多厂商都提供了类似的接口）。如果用户的输入是有风险的，就直接返回预设好的回复，不再传给模型接口生成回复。后处理是指对模型生成的内容进行风险检查，如果检测到风险内容，就将该内容屏蔽，或直接返回预设好的回复。

对于前/后处理，我们可以只做一侧，也可以两侧同时做，区别在于多了一次接口调用。多一次接口调用意味着响应时间变得更长，同时这方面的费用也会增加。另外需要注意的是，如果是流式输出，则由于Token是一个一个“吐”出来的，因此可能需要在一句话结束时就对它进行风险检查。

关于风险检查模块或接口，检查结果可能只是简单的一个布尔值，表示是否有风险，也可能得到更加细致的信息，提示调用方是哪方面的风险，以及置信度有多高。我们可以根据实际需要选择合适的接口，或自主研发相应的模块。

##### 6.2.2 提示词
大语言模型本身具备极强的理解能力，我们可以在每次输入的提示词中描述对输出内容的要求。对于包含上下文的场景（比如之前介绍过的文档问答），可以限制大语言模型必须基于给定上下文进行回复。另外，也可以在提示词（注意，是提示词而不是接口参数）中限制输出的长度，虽然有时候不太管用，但这对于理解能力极强的大语言模型来说还是有效果的。而且，限制输出的长度还能节省费用。

通过提示词控制所生成内容的操作比较简单，一般情况下能达到要求，但依然有三种情况需要特别注意。

- 给定的上下文本身就是风险内容。此时，模型基于上下文给出的回复自然也有可能是风险内容。

- 模型本身的知识是不完备的，它并不一定能理解所有的风险，尤其是每个不同用户期望和认识的“风险”。此时，模型认为自己的输出没问题，但不适合我们的场景。 

- 不排除有人恶意引导模型输出风险内容。和网络骇客一样，大语言模型骇客也会利用模型缺陷对模型进行攻击。

最后要强调的是，即使我们不考虑上面这三种情况，并且在提示词中也做了各种控制，模型也依然有可能输出我们不期望的回复，尤其是当类似temperature这种控制所生成内容多样性的参数被设置得不太保守时。因此，根据实际情况，我们有可能需要将提示词和其他方法结合起来使用。

##### 6.2.3 可控文本生成

NLP有一些细分领域，其中一个细分领域叫可控文本生成（controllable text generation, CTG），主要研究如何控制模型的输出，让其更加可控，即输出我们想要的内容，不输出我们不想要的内容。可控文本生成的方法有很多，由于涉及模型训练或微调，因此我们不过多深入介绍，感兴趣的读者可以查找并阅读相关文献。简单来看，可控文本生成的方法主要分为三类。

- **使用控制Token**：在文本的开头增加一个控制生成属性的Token，这里的属性可以是情感倾向、主题、风格等。基于提示
