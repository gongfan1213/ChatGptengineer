### 第5章 复杂推理——更加像人一样思考

在之前的章节中，我们学习了如何使用大语言模型来处理NLU和文本生成任务。目前的大语言模型已经能够轻松应对日常任务，但是当任务的复杂度超过一定的阈值时，直接使用这些模型可能无法完成任务。本章将在现有任务的基础上，探讨如何让大语言模型更好地处理复杂任务。复杂推理是一个全新且备受关注的方向，它可以使大语言模型在复杂情况下能够有效地处理任务，从而可能改变人机交互方式，重塑整个计算机生态。

思考是人类特有的能力，也是奠定人类繁荣文明的关键基石。而在思考过程中，推理是最为复杂的一种形式，具备一定的规则和逻辑性，以形式上的规范和严谨为特点。为了充分发挥大语言模型的潜力，必须赋予它们思考和推理的能力。复杂推理不仅是大语言模型的独特能力，也是大模型与小模型的主要区别所在。

本章主要探讨和应用现有大语言模型的复杂推理能力，而不是探究如何构建具有强大复杂推理能力的模型。下面，我们将介绍一系列技术和方法，重点在于如何激发和提升这些大语言模型的复杂推理能力，以改进它们在处理复杂任务时的性能。

### 5.1 什么是复杂推理

复杂推理是指在处理复杂任务时运用逻辑能力、推断能力和推理能力，从已知信息中得出新的结论或解决问题的过程。它涉及对多个变量、关系、条件进行考虑和分析，并使用逻辑、归纳、演绎等思维方式进行推断和推理。复杂推理通常需要结合不同的信息源，进行推理链的构建，以推导出更深层次的关联和推断结果。这种推理过程常见于解决复杂问题、推断未知信息或处理抽象概念等情形，需要高级思维能力和推理技巧。以下是一些常见的思维技巧和方法。

- **逻辑推理**：运用逻辑规则和关系来推理信息，包括演绎推理（从一般原理推导出特定结论）和归纳推理（从特定案例推导出一般原理）。

- **分析和综合**：将问题分解成更小的部分，对每一部分进行分析，并最终综合各个部分的结果以得出整体结论。 

- **比较或对比**：对不同的选项、观点或解决方案进行比较或对比，以确定它们之间的相似性、差异性和优劣势。

- **推断和假设**：基于已知信息进行推断，并根据可能性进行假设，以推导出缺失或未知的信息。 

- **反向推理**：从所需的结论或目标出发，逆向思考并推导出达到该结论或目标所需要的前提条件或步骤。 

- **模式识别和归纳**：寻找模式、趋势或共性，并基于这些发现进行归纳推理，以推断未知情况或扩展到新的情境。 

- **问题解决策略**：运用各种问题解决技巧，如分析图表、制定假设、进行试错等，以解决复杂问题。 

- **反思和调整**：对推理过程进行反思和调整，检查和修正可能存在的偏见、错误或不完整的推理。

以前，我们通常认为复杂推理是人类的专属能力，但是随着现代人工智能和机器学习技术的发展，我们发现人工智能在复杂任务中展现出巨大的潜力。特别是大语言模型诞生和与之伴随的涌现能力被发现后，大语言模型对复杂任务的理解和推理能力异常卓越，超出了我们以往的想象。现在，大语言模型的复杂推理能力正受到越来越多研究者的关注。在GPT-4的发布博客中，作者这样写道：“In a casual conversation, the distinction between GPT-3.5 and GPT-4 can be subtle. The difference comes out when the complexity of the task reaches a sufficient threshold—GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.”中文意思如下：“在一次随意的对话中，GPT-3.5和GPT-4的区别可能不太明显。但是，当任务的复杂度达到足够的阈值时，差异就会显现出来——GPT-4比GPT-3.5更可靠、更有创造力，所能够处理的指令比GPT-3.5更细腻。”

此外，开发出一个具有强大复杂推理能力的模型对于未来适配各类下游任务具有重要意义。尽管目前人工智能在复杂推理方面取得了不错的进展，但仍然和人类存在很大的差距，特别是在以下几个方面，人类仍然具备独特的优势：理解和处理模糊性、深层次理解、创造性思维、具体领域知识、伦理和价值判断等。

尽管目前人工智能在处理复杂任务时存在一些局限性，但相信随着技术的不断发展和研究的深入，人工智能在这方面的能力有望持续得到提升。我们期待更多的研究者和爱好者能够参与进来，共同探索并克服这些局限性，使机器能够更好地模拟和执行复杂任务，使人工智能的思考能力更接近人类。

目前，学术界和工业界正集中力量开发具有强大复杂推理能力的大语言模型，这是一个快速发展的领域，对算力和资源的要求极高。开发一个大语言模型的成本对于一般用户和开发者来说难以承担，相比之下，研究如何更有效地利用现有的大语言模型来进行复杂推理显得更为可行和友好。因此，本书更加关注于指导用户和开发者如何利用现有的大语言模型，激活它们的复杂推理能力，以进一步推动大语言模型的开发和应用。

### 5.2 复杂推理能力的激活和改善

在讨论如何激活和改善大语言模型的复杂推理能力之前，我们首先需要对大语言模型的推理能力有一个初步的了解。为此，笔者选了几个经典的推理问题，并对ChatGPT进行测试，以初步评估其推理能力。

#### 5.2.1 初步评估ChatGPT的推理能力

**问题5-1：演绎推理**

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "大前提：人类都是凡人 \n 小前提：苏格拉底是人 \n 结论："}
    ],
    temperature=0
)
print(response["choices"][0]["message"]["content"])
```
ChatGPT输出如下。

> 苏格拉底是凡人。

可以看到，ChatGPT能够根据提供的前提和问题，给出相应的结论或回答。

**问题5-2：归纳推理（一）**

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "西瓜是甜的，香瓜是甜的，所以叫“瓜”的蔬果都应该 \n 结论："}
    ],
    temperature=0
)
print(response["choices"][0]["message"]["content"])
```

ChatGPT输出如下。
> 是甜的。

可以看到，ChatGPT能够根据已知的信息进行推理并给出合理的结论。

**问题5-3：归纳推理（二）**

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "6，9，12，15，？\n 结论："}
    ],
    temperature=0
)
print(response["choices"][0]["message"]["content"])
```

ChatGPT输出如下。
> 18。

这说明ChatGPT能够解答简单的数学问题。

**问题5-4：溯因推理**
```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "大前提：罐子里装满了黄色的弹珠 \n 小前提：鲍勃手里有一颗黄色的弹珠 \n 问题：鲍勃手里的弹珠来自哪里？"}
    ],
    temperature=0
)
print(response["choices"][0]["message"]["content"])
```
ChatGPT输出如下。
> 无法确定，因为罐子里装满了黄色的弹珠，鲍勃手里的黄色弹珠可能来自罐子，也可能来自其他地方。

这说明ChatGPT能够在给定的前提和问题下，表达出对问题的不确定性。

综上所述，ChatGPT在这些对话交互中展现出了一定的推理能力，它能根据提供的信息给出合理的回答或结论。然而，需要注意的是，ChatGPT的回答受到模型的限制，它在某些情况下可能给出不准确或不完整的回答。

#### 5.2.2 复杂推理能力的激活

思维链（chain-of-thought，CoT）是一系列有逻辑关系的思考步骤，它们构成了完整的思考过程。通过一连串相关问题或句子的提示，我们可以逐步引导大语言模型进行连贯的推理和推断。这种链式思维提示激发了模型的推理能力，在给定上下文中实现了连续思考和推论，还能帮助模型填补空缺、回答问题，使其在复杂推理（如逻辑推理、因果推断、条件推理）任务中生成准确、连贯的输出，展示出强大的推理和理解能力。思维链是一种有效引导大语言模型进行连贯推理和推断的方法，它揭示了大语言模型在处理复杂任务时的卓越性能和涌现能力。

GSM8K数据集最初由OpenAI于2021年10月发布，由8500个高质量的小学数学问题组成，这些问题均由人类撰写。当时，OpenAI使用第一版GPT-3模型，在整个训练集上进行了微调，但准确率仅约为35%。这个结果让人感到悲观，因为它似乎表明大语言模型的性能受到了缩放规律的约束：随着模型规模呈指数级增长，其性能仅呈线性增长。因此，人们提出了以下观点：“参数规模为1750亿的大语言模型似乎需要至少额外两个数量级的训练数据才能达到80%的求解率。”

2022年1月，Wei等人利用参数规模为5400亿的PaLM模型，仅仅使用8个思维链提示示例，就将准确率从原来的18%提高到了56.6%，无须增大训练集的规模 ①。随后，2022年3月，Wang等人使用相同的PaLM模型，通过多数投票的方法将准确率提升至74.4% ②。进一步地，2022年10月，Fu等人利用复杂思维链技术，在参数规模为1750亿的Codex上实现了82.9%的准确率 ③。我们从这些进展中可以看到技术方面已经取得了巨大进步。

有些读者可能认为大语言模型只能解决小学数学问题，这并不足以代表什么
- ① Wei J, Wang X, Schuurmans D, et al. Chain of Thought Prompting Elicits Reasoning in Large Language Models[J]. 2022. DOI:10.48550/arXiv.2201.11903.
- ② Wang X, Wei J, Schuurmans D, et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models[J]. 2022. DOI:10.48550/arXiv.2203.11171. 
- ③ Fu Y, Peng H, Sabharwal A, et al. Complexity-Based Prompting for Multi-Step Reasoning[J]. 2022. DOI:10.48550/arXiv.2210.00720. 
