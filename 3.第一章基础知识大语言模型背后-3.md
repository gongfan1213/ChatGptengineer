GPT-1和BERT一样，用的是下游任务微调范式，也就是在不同下游任务数据上微调预训练模型，如图1-8所示。

![image](https://github.com/user-attachments/assets/4a153483-a7ba-4c51-b4dd-a015f3e2f61f)


**图1-8 GPT-1基本结构和下游任务微调范式（摘自GPT-1论文“Improving Language Understanding by Generative Pre-Training”）**
（此处未详细描述图中内容）

关于图1-8左边的GPT-1基本结构，我们在前面已经介绍过了，用的是Transformer的解码器，不过这里因为没有编码器，所以不需要有和编码器交互的多头注意力模块。现在重点看看图1-8的右边，这是GPT-1在各种下游任务上的处理流程。简单来说，就是针对不同的任务构造不同的输入序列，然后丢给GPT-1获取Token或句子的Embedding表示，再通过Linear+Softmax输出结果。Linear是一种最基础的网络结构，也就是线性映射，这里用于维度转换，转为输出需要的大小。Softmax主要用来把输出映射到概率分布（概率和为1）。这种拼接输入的方法在当时非常流行，紧跟其后的BERT也使用类似的方式，并引领了一个时代，直至ChatGPT的出现让我们进入大语言模型时代（不过，针对很多传统NLP任务BERT依然具备优势）。统一的处理方法能够减小不同任务对模型的适配难度。因此不管什么任务，都想方设法将其变成一个序列就行，比如在图1-8中，相似匹配就是把两句话直接拼接起来，预测它们是否相似（输出标签为1或0）。

GPT-1的这篇论文还有几个点在当时看起来可能没什么感觉，现在回看却有点意思。第一，预训练模型中的每一层（图1-8中的淡黄色模块）都包含用于解决目标任务的有用功能，多层（意味着模型更深）有更多能力；第二，随着参数的增加，零样本获得更好的性能。简单总结就是，模型大了不仅能学到更多知识，有助于解决下游任务，还表现出了零样本能力。这里的零样本（zero-shot）是指直接给模型输入任务，让它输出任务结果。与此类似的还有少样本（few-shot）和单样本（one-shot），即给模型提供一些（或一个）示例，然后给出任务，让它输出任务结果。

有了上面的结论，你是不是想看看更多层（更多参数）的表现如何？于是半年后，GPT-2来了，参数量从GPT-1的1.1亿增加到了15亿，增长了十几倍。更有意思的是，GPT-1的博客文章“Improving language understanding with unsupervised learning”中有一个“未来工作列表”，排在第一位的就是扩大规模，还有两个分别是提升微调，以及更好地理解为什么生成式预训练能提升NLU能力。

GPT-1发布于2018年6月，GPT-2发布于2019年2月，GPT-2是GPT-1的升级版，主要在两个方面进行进一步研究：首先是扩大规模，然后是零样本。如果说GPT-1是观察到了“规模大、能力强的零样本”这个现象，那么GPT-2就是进一步研究这个现象。其结果自然是，模型越来越大，参数越来越多，能力越来越强。GPT-2进一步验证了GPT-1的想法，下一步要做的就是继续扩大规模。

不过且慢，在此之前，我们不妨看一下GPT-2中的Token生成策略，也就是生成下一个Token的方法。前面介绍过比较优秀的集束搜索，不过它有两个比较明显的问题：第一是生成的内容容易重复，第二是高质量的文本和高概率并不一定相关（有时甚至完全没有关系）。简单来看，这两个问题其实可以归结为一个问题：生成的内容依然确定性太大。人们更希望有“不一样”的内容，而不是完全可预测的内容，比如张爱玲说过，“孤独的人有他们自己的泥沼”，这种独一无二的文字用高概率的词大概率是得不到的。

现在我们介绍一种基于采样的方法，简单来说，就是根据当前上下文得到的概率分布采样下一个Token。这里可以用一个温度（temperature）参数调整输出的概率分布，参数值越大，分布看起来就越平滑，也就是说，高概率和低概率的差距变小了（对输出不那么确定）；当然，这个参数值越小的话，高概率和低概率的差距就会更明显（对输出比较确定）；如果这个参数值趋近于0，那就和贪心搜索一样了。请看下面的代码示例。
```python
import numpy as np

np.random.seed(42)
logits = np.random.random((2, 4))
logits /= temperature
scores = np.exp(logits)
probs = scores / np.sum(scores, axis=1, keepdims=True)
```
我们让温度参数分别取0.1和0.9，结果如下。
```python
# temperature=0.1
array([[0.003, 0.873, 0.098, 0.026],
       [0.001, 0.001, 0. , 0.998]])

# temperature=0.9
array([[0.176, 0.335, 0.262, 0.226],
       [0.196, 0.196, 0.176, 0.432]])
```
以第一行为例，当温度为0.1时，概率最大值为0.873；当温度为0.9时，概率最大值依然在同样位置（这是必然的），但值变为0.335。而且，你也可以很明显地看出来，当温度为0.9时，4个数字看起来更加接近。

还有一个重复惩罚参数（repetition_penalty），它可以在一定程度上避免生成重复的Token。它和温度参数类似，只不过是将温度放到了“已生成的Token”上。也就是说，如果有Token之前已经生成过了，我们就会在生成下一个Token时对那些已生成的Token的分数进行平滑，让它们的概率不那么大。所以，这个参数值越大，越有可能生成和之前不重复的Token。

除了这些技巧，2018年的一篇论文“Hierarchical Neural Story Generation”另外介绍了一种新的采样方案，它很简单也很有效果，它就是GPT-2里使用到的Top-K采样。简单来说，就是在选择下一个Token时，从Top-K（根据概率从大到小的前K个）个Token里面选。这种采样方案不错，不过还有个小问题，就是Top-K采样其实是一种硬截断，根本不管第K个概率是高还是低。在极端情况下，如果某个词的概率是0.99（剩下的所有词加起来才0.01），K稍微大一点就必然会囊括进来一些概率很低的词。这会导致生成的内容不连贯。

于是，2019年的一篇论文“The Curious Case of Neural Text Degeneration”提出了另一种采样方案——Top-P采样，GPT-2里也有用到这种采样方案。这种采样方案是从累积概率超过P的词里进行选择。这样，对于概率分布比较均匀的情况，可选的词就会多一些（可能几十个词的概率和才会超过P）；对于概率分布不均匀的情况，可选的词就会少一些（可能两三个词的概率和就超过了P）。

Top-P采样看起来更优雅一些，两者也可以结合使用。不过在大部分情况下，当我们需要调参数的时候，调一个参数就好，包括前面的温度参数。如果要调多个参数，请确保理解每个参数的作用。最后需要说明的是，任何一种采样方案都不能100%保证每一次生成的效果都很好，也没办法完全避免生成重复的句子，也没有任何一种采样方案在任何场景下都适用。读者在使用时需要根据实际情况多尝试，选出效果最好的配置。不过，建议读者从官方给的默认参数开始尝试。

GPT-3发布于2020年7月，这在当时也是个大新闻，因为它的参数已经达到其他任何模型在当时都望尘莫及的量级——1750亿，是GPT-2的100多倍，没有开源。GPT-3既然有零样本能力，那能不能不微调呢？碰到一个任务就微调，这多麻烦。对于人来说，只要几个例子（少样本）和一些简单的说明，就可以处理任务了。怎么办？GPT-2不是进一步确认了零样本能力吗？继续加大参数量，于是就有了GPT-3。也就是说，各种任务来吧，不调参数，顶多就要几个例子（预计下一步连例子也不要了），GPT-3就能帮你完成它们。其实现在回头看，这篇论文是具有里程碑意义的，因为它从根本上触动了原有的范式，而且是革命性的触动。关于这一点，感兴趣的读者可以进一步阅读笔者的一篇文章《GPT-3和它的In-Context Learning》。现在回忆，1750亿的参数量在当时看太大了，而且也太贵了（几百万美元），一般的单位和个人根本负担不起。关于这一点，不光小部分人没意识到，可能是除了OpenAI团队之外的整个世界都没意识到。请看图1-9，横坐标是样本数量，纵坐标是精准度。图1-9提供了如下信息。
 - x-shot（x表示zero、one、few）在不同参数规模下差别巨大，大语言模型有超能力。
 - 在大语言模型下，单样本效果明显大幅提升，增加提示词会进一步大幅提升效果。
 - 少样本的边际收益在递减。大概在8样本以下时，提示词作用明显，但从单样本到8样本，提示词的效果提升幅度也在递减。当超过10样本时，提示词基本就没有作用了。

![image](https://github.com/user-attachments/assets/7abbfbf5-230a-499f-8509-36fdd74bdd24)


**图1-9 x-shot在不同参数规模下的表现（摘自GPT-3论文“Language Models are Few-Shot Learners”）**
（此处未详细描述图中内容）

总而言之，大语言模型具有In-Context（上下文）学习能力，这种能力使得它不需要针对不同任务再进行适应性训练（微调），大语言模型用的就是它自己本身的理解力。这本应该很让人震惊（甚至有一点惊恐），不过大家可能都先被它的价格和规模震惊到了。接下来，我们再直观地感受一下利用这种In-Context学习能力完成任务的方式，如图1-10所示。

图1-10右边的微调方式需要先根据训练样本更新模型参数，之后再进行预测。图1-10左边的三种方式都利用了大语言模型（large language model, LLM）的In-Context学习能力，不需要更新模型，而且看起来也都不复杂，只需要按照格式把输入构建好，然后传给模型进行预测就可以了。这也是本书写作的初衷之一——人工智能已经平民化，只要有手（可能以后不用手也行），通过使用LLM就可以做出人工智能应用了。不过这里有一点需要说明，为了简便，图1-10中的样本都比较简单，但实际中的样本一般是完整的句子。

![image](https://github.com/user-attachments/assets/691d198a-b47e-4cb8-9aa7-4baeaf04f0d5)


**图1-10 使用In-Context学习能力和微调完成任务（摘自GPT-3论文“Language Models are Few-Shot Learners”）**
（此处未详细描述图中内容）

 - 零样本（zero-shot）：模型根据任务描述直接预测答案。示例：将英文翻译成中文：rich knowledge => 丰富的知识
 - 单样本（one-shot）：除了任务描述还有一个示例。示例：将英文翻译成中文：eat apple => 吃苹果；rich knowledge => 丰富的知识
 - 少样本（few-shot）：除了任务描述还有少量示例。示例：将英文翻译成中文：eat apple => 吃苹果；my teacher => 我的老师；rich knowledge => 丰富的知识
 - 微调：模型根据训练样本更新参数。示例：样本1：hello guy => 你好小伙；样本2：peppermint => 薄荷糖；样本N：lovely girl => 可爱女孩；新输入：rich knowledge => （经模型预测）

最后值得一提的是GPT-3论文中的展望，在GPT-3论文的“局限”小节中，作者提出了GPT-3目前的一些问题，其中有两点需要特别指出，因为它们是下一代InstructGPT（也是ChatGPT的姐妹版）以及更高级版本的方向。
 - 自监督训练（也就是语言模型一般的训练方法）范式已到极限，新的训练方法迫在眉睫。未来的方向包括：从人类那里学习目标函数、强化学习微调或多模态。
 - 不确定少样本是在推理时学习到新的任务，还是识别出来了在训练时学到的任务。最终，甚至不清楚人类从零开始学习与从之前的样本中学习分别学到了什么。准确理解少样本的工作原理是未来的一个方向。

上面的第一点在1.3.3节就会提到，这里主要说说第二点。当我们给出一些示例（少样本）时，我们还无法精准确定是在推理时“学习”到新任务的处理方法（在这种情况下，没有示例就没有能力；这里的“学习”要打引号，因为它不调整参数），还是在训练时就已经具备了这个能力，示例只是让它“回想”起之前学的东西。这里有点绕，拿人来举例，可能不太恰当，但能大致说明问题。假设当你读到一首诗时，自己也诗兴大发写了一句诗。你说这句诗是因为你读到这首诗时“领悟”到的，还是你本来就有这个积累（记忆），现在只是因为读这首诗而被激发出来？这可能涉及大脑、思维、意识等领域知识，而人类至今也没有弄清楚它们的原理，所以我们现在还不知道答案。 
