### 4.3.3 基于OpenAI接口的文本纠错实验

我们直接尝试使用ChatGPT来进行文本纠错，如下所示。

```python
def correct_text(text):
    content = f"请对以下文本进行文本纠错：\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": content}]
    )
    corrected_text = response.get("choices")[0].get("message").get("content")
    return corrected_text

text = "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课乘吧！"
output_text = correct_text(text)
# 纠错文本
output_text == "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课程吧！"
```

类似于上文的修改位置查看脚本，我们可以使用Redlines函数来实现类似的功能。具体来说，就是对比输入文本和输出文本之间的差异，用画线与标红来表示差异点。可以看到，ChatGPT的纠错效果很不错，连中英文标点符号都识别出来了。

```python
from redlines import Redlines
from IPython.display import display, Markdown

diff = Redlines(" ".join(list(text)), " ".join(list(output_text)))
display(Markdown(diff.output_markdown))
# 查看修改，为了便于展示，将画线展示为 []
output_text="大家好 [,]，一起来参加Datawhale的“ChatGPT使用指南”组队学习课 [乘] 程吧！"
```

### 4.4 机器翻译

#### 4.4.1 什么是机器翻译

机器翻译又称为自动翻译，是利用计算机将一种自然语言（源语言）转换为另一种自然语言（目标语言）的过程。据不完全统计，世界上约有7000种语言，两两配对约有4900万种组合，这些语言中又不乏一词多义等现象。因此，能够使用更少的标注数据，或者无监督地让计算机真正地理解输入语言的含义，并信、达、雅地转换为输出语言，是学者们历来的研究重心。

众所周知，机器翻译一直是NLP领域备受关注的一个研究方向，也是NLP技术最早崭露头角的任务之一。如今，市面上的机器翻译工具层出不穷，如大家常用的百度翻译、谷歌翻译，乃至科幻电影里才有的人人工智能同声传译，如讯飞听见同传。简单来说，机器翻译可以划分为通用领域（多语种）、垂直领域、术语定制化、领域自适应、人工适应、语音翻译等。

#### 4.4.2 常见的机器翻译技术

从机器翻译的发展历程来看，它主要经历了如下三个阶段。

- 基于规则的机器翻译技术。

- 基于统计的机器翻译技术。 

- 基于神经网络的机器翻译技术。

1. **基于规则的机器翻译技术**

基于规则的机器翻译需要建立各类知识库，以及描述源语言和目标语言的词法、句法及语义知识。简单来说，就是建立一个翻译字典与一套语法规则，先翻译重要的词汇，再根据目标语言的语法将词汇拼接成正确的句子。这种方法需要丰富且完善的专家知识，且无法处理未在字典及规则中出现过的情况。

2. **基于统计的机器翻译技术** 

基于统计的机器翻译是从概率的角度实现翻译的，其核心原理是，对于源语言中的每个词r，先从词表中找出最可能与之互译的词t，再调整词t的顺序，使其合乎目标语言的语法。假设我们拥有一个双语平行语料库，可以将源词与目标词在两个句子中共同出现的频率作为两个词表示的是同一个词的概率。比如将“我对你感到满意”翻译成英文，假设中文的“我”和英文的“I”“me”“I'm”共同出现的概率最高，即它们表示的是同一个词的概率最高，则将其作为候选词，再根据英文语法挑选出“I'm”是最佳的翻译词。这种方法被称为基于词对齐的翻译。但是由于短语和语法的存在，有时并不是一个词表示一个含义，而是由多个词共同组合成一个短语来表示一个含义，如英文的“a lot of”共同表示了中文的“很多”。因此，将翻译的最小单位设计成词显然是不符合语法的，于是后来又延伸出基于短语的翻译方法——将最小翻译单位设计成连续的词串。

3. **基于神经网络的机器翻译技术**

2013年，一种用于机器翻译的新型端到端编码器 - 解码器架构问世，CNN用于隐含表征挖掘，RNN则用于将隐含向量转换为目标语言，开启了基于神经网络的机器翻译时代。后来，Attention、Transformer、BERT等技术被相继提出，大大提升了机器翻译的质量。

以下是一个基于Transformer实现机器翻译的简单示例。


```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-zh-en")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-zh-en")

text = "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课程吧！"

inputs = tokenizer(text, return_tensors="pt", )
outputs = model.generate(inputs["input_ids"], max_length=40, num_beams=4, early_stopping=True)
translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
# 翻译文本
translated_sentence == "Hey, guys, let's join the ChatGPT team at Datawhale."
```

翻译的效果看起来不是特别好。

#### 4.4.3 基于OpenAI接口的机器翻译实验

让我们来试试ChatGPT的效果。

1. **简单上手版：短文本英译中**

```python
def translate_text(text):
    content = f"请将以下中文文本翻译成英文：\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": content}]
    )
    translated_text = response.get("choices")[0].get("message").get("content")
    return translated_text

text_to_translate = "大家好，一起来参加Datawhale的“ChatGPT使用指南”组队学习课程吧！"
translated_text = translate_text(text_to_translate)
# 翻译文本
translated_text == "Hello everyone, let's join the team learning course of \"ChatGPT User Guide\" organized by Datawhale together!"
```
可以看到，ChatGPT明显比刚才的模型效果要好，不仅语义正确，还将课程名翻译得更加具体了。


2. **进阶深度版：长文本英译中**

在以上内容中，我们更多地了解了如何对短文本实现摘要、纠错、翻译等功能。目前，ChatGPT仅支持有限个词汇的输入。但是在实际场景中，特别是对于翻译问题，往往需要对很长的输入文本进行处理。一个简单的想法是，对输入文本进行切割，每次切割出不超过模型所能接受的最大单词数的文本进行处理，并保存结果输出，最后将所有的结果输出拼接到一起，得到最终结果。

下面我们以翻译《哈利波特》英文原著为例，学习如何处理长文本翻译任务。

首先导入《哈利波特》英文原著。

```python
with open("dataset/哈利波特1-7英文原版.txt", "r") as f:
    text = f.read()
# 整套书的字符数
len(text) == 6350735
```

整套书的字符数约635万，但我们知道，ChatGPT的接口调用费用是根据Token数来定的，我们可以简单地使用tokenizer来统计全书Token数。

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")  # GPT-2的tokenizer和GPT-3是一样的
token_counts = len(tokenizer.encode(text))
# 整套书的Token数
token_counts == 1673251

# ChatGPT的接口调用费用是每一千个Token 0.01美元，因此可以大致计算翻译整套书的价格
translate_cost = 0.01 / 1000 * token_counts
# 翻译整套书的价格
translate_cost == 16.73251
```
在这里，我们使用GPT2Tokenizer统计整套书的Token数，并根据ChatGPT的接口调用费用来估计翻译整套书的价格。得到翻译整套书大约需要人民币115元（按照本书写作时的人民币兑美元汇率计算得出），这有点贵了，我们试着只翻译第一册。
```python
end_idx = text.find("2.Harry Potter and The Chamber Of Secrets.txt")
text = text[:end_idx]
# 第一册的字符数
len(text) == 442815

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
token_counts = len(tokenizer.encode(text))
# 第一册的Token数
token_counts == 119873

translate_cost = 0.01 / 1000 * token_counts
# 翻译第一册的价格
translate_cost == 1.19873
```
只翻译第一册大约需要人民币9元，相对还算实惠。

类似ChatGPT这样的大语言模型一般对所输入Token的长度有限制，因此可能无法直接将包含12万个Token的文本全部输进去。我们可以使用一种简单的方法：将文本分成若干份，对每一份使用ChatGPT进行翻译，最后将所有翻译结果拼接起来。

当然，随意地切割文本是不合理的，在保证每一份文本的长度小于最大限制长度的条件下，我们最好还能保证每一份文本本身的语义连贯性。如果从一个句子的中间将上下文拆成两块，翻译时容易出现歧义。一个比较直观的想法是，将每个段落当成一个文本块，每次翻译一段。但是第一册的段落非常多，有3000多段，而每段文本的单词相对较少，最长的段落仅有275个单词。显然，一段一段地翻译会降低翻译的效率。同时，每段文本的上下文较少，这会导致翻译错误的可能性上升。

```python
paragraphs = text.split("\n")
# 段落数
len(paragraphs) == 3038

ntokens = []
for paragraph in paragraphs:
    ntokens.append(len(tokenizer.encode(paragraph)))
# 最长段落的Token数
max(ntokens) == 275
```

因此，我们选定一个阈值，如500，每次加入一个文本段落，如果Token总数超过500，则开启一个新的文本块。
```python
def group_paragraphs(paragraphs, ntokens, max_len=1000):
    """
    合并短段落为文本块，用于丰富上下文语境，提升语义连贯性，并提升翻译效率。
    :param paragraphs: 段落集合
    :param ntokens: Token数集合
    :param max_len: 最长文本块的Token数
    :return: 组合好的文本块
    """
    batches = []
    cur_batch = ""
    cur_tokens = 0

    # 对每个文本段落进行处理
    for paragraph, ntoken in zip(paragraphs, ntokens):
        if ntoken + cur_tokens + 1 > max_len:  # "1" 指的是"\n"
            # 如果在加入这段文本后，总Token数超过阈值，则开启新的文本块
            batches.append(cur_batch)
            cur_batch = paragraph
            cur_tokens = ntoken
        else:
            # 否则将段落插入文本块中
            cur_batch += "\n" + paragraph
            cur_tokens += (1 + ntoken)
    batches.append(cur_batch)  # 记录最后一个文本块
    return batches

batches = group_paragraphs(paragraphs, ntokens, max_len=500)
# 文本块数
len(batches) == 256

new_tokens = []
for batch in batches:
    new_tokens.append(len(tokenizer.encode(batch)))
# 最长文本块的Token数
max(new_tokens) == 500
```
经过段落的重新组合，我们得到了256个文本块，其中最长文本块的Token数为500。

我们在实操中发现，由于受到接口使用速率的限制，用ChatGPT翻译长文本很慢，这里改用Completion接口来实现。
```python
def translate_text(text):
    content = f"请将以下英文文本翻译成中文：\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": content}]
    )
    translated_text = response.get("choices")[0].get("message").get("content")
    return translated_text
```
```python
def translate_text(text):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=f"请将以下英文文本翻译成中文：\n{text}",
        max_tokens=2048
    )
    translate_text = response.choices[0].text.strip()
    return translate_text
```
接下来，我们对每个文本块进行翻译，并将翻译结果拼接起来。
```python
from tqdm import tqdm

translated_batches = []
translated_batches_bak = translated_batches.copy()
cur_len = len(translated_batches)
for i in tqdm(range(cur_len, len(batches))):
    translated_batches.append(translate_text(batches[i]))
```
有时候，由于网络问题，可能会出现连接中断或连接超时错误。解决方法有两种：一种方法是从断点处开始重连；另一种方法是加入重试机制，如果失败，则尝试自动重连。以下脚本会在失败后随机等待一段时间并重连，如果重试6次仍失败，则整个任务失败。
```python
from tenacity import retry, stop_after_attempt, wait_random_exponential

@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))
def translate_text(text):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=f"请将以下英文文本翻译成中文：\n{text}",
        temperature=0.3,
        max_tokens=2048
    )
    translate_text = response.choices[0].text.strip()
    return translate_text
```
```python
for i in tqdm(range(len(batches))):
    translated_batches.append(translate_text(batches[i]))
```
保存结果至文本文件，这样我们便有了一份完整的译文。
```python
result = "\n".join(translated_batches)
with open("dataset/哈利波特1中文版翻译.txt", "w", encoding="utf-8") as f:
    f.write(result)
```

### 4.5 本章小结

在本章中，我们主要学习了ChatGPT在NLG任务中的应用。我们首先简单介绍了NLG任务的一些基础知识，然后对文本摘要、文本纠错、机器翻译三个具体的任务分别进行了介绍。对于文本摘要任务，我们对比了传统方法与ChatGPT的输出结果，并基于ada模型对自定义语料库进行微调。对于文本纠错任务，我们同样对比了传统方法与大语言模型的输出结果，并基于一些工具或自定义函数实现了输出的可视化展示。对于机器翻译任务，我们一方面学习了ChatGPT在短文本翻译上的应用，另一方面通过对输入文本进行切割与组合，实现了长文本的翻译。 
