#### 1.3 ChatGPT基础
##### 1.3.1 最强表示架构Transformer设计与演变
接下来出场的是Transformer，它是一个基于注意力机制的编码器 - 解码器（encoder-decoder）架构，刚开始主要应用在NLP领域，后来横跨到语音和图像领域，并最终统一几乎所有模态（文本、图像、语音）的架构。Transformer来自谷歌公司在2017年发表的一篇论文“Attention Is All You Need”，其最重要的核心就是提出来的自注意力（self-attention）机制。简单来说，就是在语言模型构建过程中，把注意力放在那些重要的Token上。

Transformer简单来说，就是先把输入映射到编码器（encoder），这里大家可以把编码器想象成前面介绍的RNN，解码器（decoder）也可以想象成RNN。这样，左边负责编码，右边则负责解码。这里不同的是，左边因为我们是知道数据的，所以在建模时可以同时利用当前Token的历史（后面的）Token和未来（前面的）Token；但在解码时，因为是一个个Token输出来的，所以只能根据历史Token以及编码器的Token表示进行建模，而不能利用未来Token。

Transformer的这种架构从更普遍的角度来看，其实是Seq2Seq（sequence to sequence）架构，简单来说就是序列到序列的架构：输入是一个文本序列，输出是另一个文本序列。翻译就是一个很好的例子，如图1-6所示。

**图1-6 Seq2Seq架构示意图（摘自GitHub的“google/seq2seq”项目）**
（此处未详细描述图中内容）

![image](https://github.com/user-attachments/assets/43e7c546-6aeb-4da0-893d-f4620777112f)


刚刚已经讲了，编码器和解码器可以采用RNN，编码器这一侧的每个Token都可以输出一个向量表示，而这些所有Token的输出向量都可以在处理后作为整句话的表示。说到这里，整句话又怎么表示呢？前面曾提到，对于RNN这种结构，可以把最后一个Token的输出作为整个句子的表示。当然，很符合直觉的，你也可以取每个词向量的平均值。除了平均值，也可以求和、取最大值等，我们就不更深入讨论了。现在重点来了，看解码的过程，仔细看，其实解码器在生成每一个Token时都用到了编码器中每一个Token的信息，以及已经生成的那些Token的信息。前面这种关注编码器中每个Token的信息的机制就是注意力（attention）机制。直观的解释，就是当生成单词“power”时，“力量”两个字会被赋予更多权重（注意力），其他情况也类似。

好了，现在让我们带着之前的记忆，看一下Transformer的整体结构，如图1-7所示。

在图1-7中，左边是编码器，一共有N个；右边是解码器，也有N个。为简单起见，我们可以假设N = 1，如此一来，图1-7的左边就是一个编码器，右边则是一个解码器。也可以把它们想象成一个RNN，这样有助于从宏观上把握。现在，我们回到现实，Transformer用到的东西其实和RNN并没有关系，这一点通过图1-7也可以很明显地看出来。Transformer主要用了两个模块：多头注意力（multi-head attention）和前馈（feedforward）网络。

对于多头注意力，我们不妨回顾一下Seq2Seq架构的注意力机制，它是解码器中的Token和编码器中每一个Token的重要性权重。多头注意力中用到了自注意力（self-attention），自注意力和刚刚讲的注意力非常类似，只不过自注意力是自己的每一个Token之间的重要性权重。简单来说，就是“一句话到底哪里重要”。自注意力机制可以说是Transformer的精髓，无论是ChatGPT还是其他非文本的大语言模型，都用到了它，它可以说是真正地“一统江湖”。多头（multi-head）简单来说，就是把刚刚的这种自注意力自己重复多次，每个头注意到的信息不一样，这样就可以捕获到更多信息。比如我们前面提到过的一句话——“人工智能能让世界变得更美好”，有的头“人工智能”注意到“世界”，有的头“人工智能”注意到“美好”……这样看起来更加符合直觉。

![image](https://github.com/user-attachments/assets/127e8405-926b-413a-a443-824c701fcbb9)


**图1-7 Transformer的整体结构（摘自论文“Attention Is All You Need”）**
（此处未详细描述图中内容）

前馈网络主要引入非线性变换，帮助模型学习更复杂的语言特征和模式。

另外，有个地方要特别注意，解码器的淡黄色模块内有一个遮盖多头注意力（masked multi-head attention），它和多头注意力的区别就是遮盖（mask）了未来Token。以本小节开头提到的翻译为例，当给定“Knowledge”生成下一个Token时，模型当然不知道下一个Token就是“is”。还记得前面讲过的学习（训练）过程吗？下一个Token是“is”，这是训练数据里的，模型输出什么要看Token最大概率是不是在“is”这个Token上，如果不在，参数就得更新。

实际上，大多数NLP任务并不是Seq2Seq架构的，最常见的任务主要包括如下几种：句子分类、Token分类（也叫序列标注）、相似匹配和文本生成。

三种应用得最为广泛。这时候，编码器和解码器就可以拆开用了。左边的编码器在把句子表示成一个向量时，可以利用上下文信息，也就是说，可以把它看作双向的；右边的解码器不能看到未来Token，一般只利用上文信息，是单向的。虽然它们都可以用来完成刚才提到的几种任务，但从效果上来说，编码器更适合非生成类任务，解码器则更适合生成类任务。在NLP领域，一般也会把它们分别叫作自然语言理解（natural language understanding, NLU）任务和自然语言生成（natural language generation, NLG）任务。上面提到的这些任务，后面都会进一步介绍，这里大致了解一下即可。

我们首先介绍NLU任务。句子分类是指给定一个句子，输出一个类别。因为句子可以表示为一个向量，所以经过张量运算后，自然可以映射到每个类别的概率分布。这和前面提到过的语言模型的做法没有本质上的区别，只不过语言模型的类别是整个词表大小，而分类的类别则要看具体的任务，有二分类、多分类、多标签分类等。Token分类是指给定一个句子，给其中的每个Token输出一个类别。这和语言模型就更像了，只不过把下一个Token换成了对应的类别，比如命名实体抽取就是把句子中的实体（人名、地名、作品名等）识别出来，对应的类别是这是名词）提取出来。如果以地名（location, LOC）举例的话，对应的类别是这样的：B-LOC（begin of LOC）表示实体开始、I-LOC（inside of LOC）表示实体中间。举个例子：“中国的首都是北京”。注意此时的Token是字，每个Token对应的类别为“B-LOC、I-LOC、O、O、O、O、B-LOC、I-LOC”，O表示Other。对于分类任务，类别一般也叫作标签。相似匹配一般指给定两个句子，输出它们是否相似，其实可以将其看作特殊的分类任务。

接下来介绍NLG任务。除文本续写外，其他常见的NLG任务还有文本摘要、机器翻译、文本改写、文本纠错等。这里Seq2Seq架构就比较常见了，体现了一种先理解再输出的思路。而纯生成类任务，比如写诗、写歌词、写小说，则几乎是纯解码器架构。此类任务稍微麻烦的是如何做自动评测，文本摘要、机器翻译、文本改写、文本纠错等任务一般都会提供参考答案（reference），可以评估模型输出和参考答案之间的重叠程度或相似程度，但纯生成类任务就有点麻烦，这个好不好有时候其实很难衡量。不过，针对有具体目标的任务（如任务型聊天机器人的回复生成），还可以设计一些诸如“是否完成任务”“是否达到目标”的评测方法。但对于没有具体目标的任务（比如闲聊），评测起来就见仁见智了，很多时候还得靠人工进行评测。

Transformer基于Seq2Seq架构，可以同时处理NLU和NLG任务，而且这种自注意力机制的特征提取能力（表示能力）很强。其结果就是NLP取得了阶段性的突破，深度学习开始进入微调模型时代，大概的做法就是，拿着一个开源的预训练模型，在自己的数据上微调一下，让它能够完成特定的任务。这个开源的预训练模型往往就是一个语言模型，在大量语料中，使用我们前面所讲的语言模型的训练方法训练而来。偏NLU领域的第一个成果是谷歌公司的BERT，相信不少人即便是不是这个行业的也大概听过。BERT就是使用了Transformer的编码器（没有使用解码器），有12个Block（图1-7左侧的淡黄色色块，每一个Block也可以叫作一层）和1亿多个参数。BERT不预测下一个Token，而是随机地把15%的Token盖住（其中80%用[MASK]替换，10%保持不变，10%随机替换为其他Token），然后利用其他没盖住的Token来预测盖住位置的Token。这其实和根据上文信息预测下一个Token是类似的，所不同的是它可以利用下文信息。偏NLG领域的第一个成果是OpenAI的GPT，GPT就是使用了Transformer的解码器（没有使用编码器），参数和BERT差不多。BERT和GPT都发布于2018年，然后分别走上了不同的道路。

##### 1.3.2 生成语言模型GPT进化与逆袭
GPT，就是ChatGPT中的那个GPT，中文叫作生成式预训练Transformer。生成式的意思就是类似于语言模型那样，一个Token一个Token地生成文本，也就是上面提到的解码器的原理。预训练刚刚也提过了，
就是在大量语料中训练语言模型。GPT模型从GPT-1到GPT-4，一共经历了5个版本，中间的ChatGPT是3.5版。GPT-1、GPT-2和GPT-3都是有论文发表的，接下来分别介绍它们的基本思想。ChatGPT没有论文发表，
不过它的姐妹版本InstructGPT有论文发表，我们放在1.3.3节介绍。GPT-4也没有论文发表，只有技术报告，不过里面并没有技术细节。因此，我们对GPT-4不做介绍，读者可以将其看作能力更强的ChatGPT升级版。 
