```python
df_train.to_json("dataset/tnews-finetuning.jsonl", orient="records", lines=True)
!openai tools fine_tunes.prepare_data -f dataset/tnews-finetuning.jsonl -q
```

转换后的数据样例如下所示。

```python
!head dataset/tnews-finetuning_prepared_train.jsonl
"""
[{"prompt":"cf生存特训：火箭弹狂野复仇，为兄弟报仇就要不死不休 ->","completion": "game"},
{"prompt":"哈尔滨 东北抗日联军纪念馆 ->","completion": "culture"},
{"prompt":"股市中，主力为何如此猖獗？一文告诉你真相 ->","completion":"finance"},
{"prompt":"天府锦绣又重来 ->","completion":"agriculture"},
{"prompt":"生活、游戏、电影中有哪些词汇稍加修改便可以成为一个非常霸气的名字？ ->", "completion":"game"},
{"prompt":"法庭上，生父要争夺孩子的抚养权，小男孩的发言让生父当场哑口无言 ->", "completion":"entertainment"},
{"prompt":"如何才能选到好的深圳大数据培训机构？ ->","completion":"edu"},
{"prompt":"有哪些娱乐圈里面的明星追星？ ->","completion":"entertainment"},
{"prompt":"东坞原生态野生茶 ->","completion":"culture"},
{"prompt":"亚冠：恒大不胜早有预示，全北失利命中注定 ->","completion":"sports"}]
"""
```

可以看到，转换后最明显的是每一个prompt的后面多了一个 -> 标记，除此之外还有下面一些调整。

- **小写**：将所有英文小写。 

- **去掉标签的news_前缀**：注意看completion的字段值，前缀都不见了，处理后的结果是一些有意义的单词，这更加合理了。 

- **在completion的字段值的前面加了一个空格**：除了去掉news_前缀，还额外加了一个空格，这是为了用空格把英文单词分开。 

- **将整个数据集划分为训练集和验证集**：训练集用来微调模型，验证集则用来评估模型性能和进行超参数调优。 


这些调整会有相应的日志输出，请注意阅读转换时的输出日志。另外，它们也都是常见的、推荐的预处理做法。 

数据准备好后，就到了第二步：微调。使用接口进行微调非常简单，一般用一行命令即可完成，甚至在页面上用鼠标单击一下就可以了。

```python
import openai
import os
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
!openai api fine_tunes.create \
    -t "./dataset/tnews-finetuning_prepared_train.jsonl" \
    -v "./dataset/tnews-finetuning_prepared_valid.jsonl" \
    --compute_classification_metrics --classification_n_classes 14 \
    -m davinci \
    --no_check_if_files_exist
```

其中，-t和 -v分别用来指定训练集和验证集。接下来那行用来计算指标。-m则用来指定要微调的模型，可以微调的模型和价格可以从官方文档中获取。最后一行检查文件是否存在，如果之前上传过文件的话，这里可以复用。 

另外值得一提的是，我们这里给的例子是Completion接口的微调，不过ChatCompletion接口也支持微调。命令执行后，将会得到一个任务ID，接下来可以用另一个接口和任务ID来获取任务的实时状态，如下所示。

```python
!openai api fine_tunes.get -i ft-QOkrWkHUOaleR6f5IQw1UpVL
```

或者用下面的接口恢复数据流。

```python
!openai api fine_tunes.follow -i ft-QOkrWkHUOaleR6f5IQw1UpVL
```

注意，一个是follow接口，另一个是get接口。读者可以通过openai api -help来查看更多支持的命令。 

建议读者过段时间通过get接口查看一下进度即可，而不需要一直调用follow接口来获取数据流。这里可能要等一段时间，等排队完成后进入训练阶段就很快了。在查看进度时，主要看status是什么状态。微调结束后，将会得到一个新的模型ID，这就是我们此次调整后的模型。另外，也可以通过下面的命令来查看本次微调的各项指标。
```python
# -i就是上面微调的任务ID
!openai api fine_tunes.results -i ft-QOkrWkHUOaleR6f5IQw1UpVL > metric.csv
metric = pd.read_csv('metric.csv')
metric[metric['classification/accuracy'].notnull()].tail(1)
```
这里主要输出训练后的损失、精度等。将精度绘制成图，如图3-1所示。

![image](https://github.com/user-attachments/assets/45174e0d-ba44-4bed-81fe-202c8fc6443b)


```python
step_acc = metric[metric['classification/accuracy'].notnull()]['classification/accuracy']
import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,6))
ax.plot(step_acc.index, step_acc.values, "k-", lw=1, alpha=1.0)
ax.set_xlabel("Step")
ax.set_ylabel("Accuracy");
```

这个精度其实是非常一般的，最高值在1200步（Step）左右，此时精度（Accuracy）达到64%左右。原因应该是我们给的语料太少。在实践中，往往数据越多、数据质量越好，相应的效果越好。 

最后是第三步：使用新模型进行推理。我们还是用刚才的例子来演示。

```python
lines[2] == {
    "label": "104",
    "label_desc": "news_finance",
    "sentence": "出栏一只羊亏损300元，究竟谁能笑到最后！",
    "keywords": "商品羊，养羊，羊价，饲羊，饲料"
}
prompt = get_prompt(lines[2]["sentence"])
prompt == """
对给定文本进行分类，类别包括：科技、金融、娱乐、世界、汽车、运动、文化、军事、旅游、游戏、教育、农业、房产、社会、股票。
给定文本：
出栏一只羊亏损300元，究竟谁能笑到最后！
类别：
"""
```
对调用接口的代码稍微调整一下，新增一个model参数。
```python
def complete(prompt: str, model: str, max_tokens: int) -> str:
    response = openai.Completion.create(
        prompt=prompt,
        temperature=0,
        max_tokens=max_tokens,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        model=model
    )
    ans = response["choices"][0]["text"].strip("\n")
    return ans
```
调用微调前的模型和前面一样，但是在调用微调后的模型时，需要注意修改提示词，如下所示。
```python
# 调用微调前的模型
complete(prompt, "text-davinci-003", 5) == "社会"
# 调用微调后的模型
prompt = lines[2]["sentence"] + " ->"
complete(prompt, "davinci:ft-personal-2023-04-04-14-51-29", 1) == "agriculture"
```
微调后的模型返回了一个英文单词，这是正常的，因为微调数据中的completion就是英文。这里是为了方便演示微调有效，读者在实际使用时务必保持统一。不过这个结果依然不是标注的“finance”，这应该是这个句子本身和“agriculture”这个类别的训练文本更加接近所致。对于这类比较特殊的样例，请务必给模型提供一定数量的类似的训练样本。 
上面介绍了主题分类的微调。实体抽取的微调也是类似的，推荐的输入格式如下。
```json
{
    "prompt":"<any text, for example news article>\n\n###\n\n",
    "completion":"<list of entities, separated by a newline> END"
}
```
示例如下所示。
```json
{
    "prompt":"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \"Nepal mutation of the so-called Indian variant\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\n\n###\n\n",
    "completion":"Portugal\nUK\nNepal mutation\nIndian variant END"
}
```
相信读者应该很容易理解，不妨对一些专业领域的实体进行微调，对比一下微调前后的效果。


### 3.3.3 智能对话：大语言模型 = 自主控制的机器人 

智能对话，有时候也叫智能客服、对话机器人、聊天机器人等。总之，它就是和用户通过聊天方式进行交互的一种技术。传统的聊天机器人一般包括三大模块。

- **自然语言理解（NLU）模块**：负责对用户输入进行理解。本章开头已经提到了，主要就是意图识别和实体抽取这两种技术。现实中可能还有实体关系抽取、情感识别等组件。 

- **对话管理（dialogue management，DM）模块**：就是在获得NLU模块的结果后，确定机器人的回复方式，也就是进行对话方向的控制。 

- **自然语言生成（NLG）模块**：就是生成最终要回复给用户的输出文本。 


聊天机器人一般包括三种类型，不同类型的技术方案侧重也会有所不同。

- **任务型机器人**：主要用来完成特定的任务，比如订机票、订餐等，这一类机器人最关键的是要获取完成任务所需的各种信息（专业术语叫作槽位）。整个对话过程其实可以被看作填槽过程，通过与用户不断地对话，获取需要的槽位信息。比如订餐，就餐人数、就餐时间、联系人电话等就是基本信息，机器人要想办法获取到这些信息，这里NLU就是关键模块。DM一般使用两种方法：模型控制或流程图控制。前者通过模型自动学习来实现流转，后者则根据意图识别进行流转控制。 

- **问答型机器人**：主要用来回复用户问题，和QA原理有点类似，平时常见的客服机器人往往就是这种类型。它们更重要的是问题匹配，DM相对弱一些。 

- **闲聊型机器人**：一般没什么实际作用。当然，还有一种情感陪伴型机器人，但它不在我们的讨论范围内。 


以上是大致的分类，真实场景中的聊天机器人往往是多种功能的结合体，更加适合从主动发起或被动接受的角度来划分。

- **主动发起对话的机器人**：一般以外呼的方式进行，营销、催款、通知等都是常见的应用场景。这种聊天机器人一般不闲聊。它们一般带着特定任务或目的走流程，流程走完就挂断结束。与用户的互动更多以QA的形式完成，因为主动权在机器人手里，所以流程一般是固定控制的，甚至QA的问题数量、回答次数也会受到控制。 

- **被动接受对话的机器人**：一般以网页或客户端的形式存在，大部分公司网站或应用首页的智能客服是常见的应用场景。它们以QA为主，辅以闲聊。稍微复杂点的场景就是上面提到的任务型机器人，也需要不断地收集槽位信息。 



在大语言模型时代，聊天机器人会有什么新变化吗？接下来，我们探讨一下这方面的内容。 

首先可以肯定的是，类似ChatGPT这样的大语言模型极大地扩展了聊天机器人的边界，大语言模型强大的In-Context学习能力不仅让使用更加简单（我们只需把历史对话分角色放进去就好了），效果也更好了。除了闲聊，问答型和任务型机器人也很擅长交互，从而更加人性化。 

我们再来具体展开说说ChatGPT可以做什么以及怎么做，下面随便举几个例子。

- **作为问答型机器人**：提供比如知识问答、情感咨询、心理咨询服务等，完全称得上诸事不决问ChatGPT。比如问它编程概念，问它如何追求心仪的女孩子，问它怎么避免焦虑，等等。它的大部分回答能让人眼前一亮。 

- **作为智能客服**：通过与企业知识库结合，可以胜任客服工作。相比QA客服，它的回答更加个性化，效果也更好。 

- **作为智能营销机器人**：智能客服更加偏向被动地为用户答疑解惑；智能营销机器人则更加主动一些，它会根据已存储的用户信息，主动向用户推荐相关产品，以及根据预设的目标向用户发起对话，它还可以同时负责维护客户关系。 

- **作为游戏中的非玩家角色（non-player character，NPC）、聊天机器人等休闲娱乐类产品**。 

- **作为教育、培训的导师**：可以进行一对一教学，尤其适合语言、编程类学习。 


这些都是ChatGPT确定可以做的，市面上也已经有很多相关的应用了。为什么大语言模型能做这些？归根结底在于其通过大规模参数学到的知识以及具备的理解力。尤其是强大的理解力，应该是决定性因素（只有知识就成了谷歌搜索引擎）。 

当然，并不是什么都要问ChatGPT，我们要避免“手里有锤子，到处找钉子”的思维方式。某位哲人说过，一项新技术的出现，短期内总被高估，长期内总被低估。ChatGPT引领的大语言模型是划时代的，但这并不意味着什么都要“ChatGPT一下”。比如，某些分类和实体抽取任务，之前的方法已经能达到非常好的效果，这时候就完全不需要替换。我们知道，很多实际任务并不会随着技术的发展而有太多的变化，比如分类任务，难道有了新的技术，分类任务就不是分类任务了吗？技术的更新会让我们的效率得到提升，也就是说，同样的任务可以更加简单和高效地完成，我们可以完成更难的任务了，但不等于任务也会发生变化。所以，一定要弄清楚任务的关键，明白手段和目的的区别。 

不过，如果要新开发一个服务，或者不了解这方面的专业知识，那么使用大语言模型接口反而可能是更好的策略。但在实际上线前，还是应该考虑清楚各种细节，比如服务不可用怎么办，并发大概多少，时延要求多少，用户规模大概多少，等等。技术方案的选型是与企业或自己的需求息息相关的，没有绝对好的技术方案，只有当下是否适合的技术方案。 

同时，要尽可能多考虑几步，但也不用太多（过度优化是原罪）。比如产品或服务的日常用户活跃数量不到几百，上来就写分布式的设计方案就有点不合适。不过，这并不妨碍我们在进行代码和架构设计时考虑扩展性，比如数据库，我们可能使用SQLite，但在代码里并不直接和它耦合，而是使用能同时支持其他数据库甚至分布式数据库的ORM（object relational mapping，对象关系映射）工具。这样虽然写起来稍微麻烦了一点，但代码会更加清晰，而且和可能会发生变化的东西解耦了。这样即便日后规模增加了，数据库也可以随便换，代码基本不用更改。 

最后，我们应该了解ChatGPT的一些局限，除了它本身的局限（后面会专门介绍）之外，在工程上至少还应该始终关注下面几个话题：响应时间和稳定性、并发和横向可扩展性、可维护性和迭代、资源和成本。只有当这些都能满足我们的期望时，才应该选择该方案。 

下面我们使用ChatGPT来实现一个简单的任务型聊天机器人。在设计阶段需要考虑以下一些因素。

- **使用目的**：首先，我们需要明确使用目的是什么，如前所述，对于不同的用途，要考虑的因素也不一样。为简单（但很实际）起见，我们以一个“订餐机器人”为例。它的功能就是简单地开场白，然后获取用户联系方式、订餐人数、用餐时间三个信息。 

- **如何使用**：使用也比较简单，主要利用ChatGPT的多轮对话能力，这里的重点是控制上下文。不过由于任务简单，我们不用对历史记录先召回再进行对话，直接在每一轮把已经获取的信息告诉ChatGPT，同时让它继续获取其他信息，直到所有信息获取完毕为止。另外，我们可以限制一下输出Token的数量（以控制输出文本的长度）。 

- **消息查询、存储**：对于用户的消息（以及机器人的回复），实践中往往需要存储起来，用来做每一轮回复的历史消息召回。而且这个日后还可能有其他用途，比如使用对话记录对用户进行画像，或者当作训练数据。存储可以直接放到数据库中，也可以传到类似于ElasticSearch这样的内部搜索引擎索引中。 

- **消息解析**：消息的解析可以实时进行（不一定要用ChatGPT）或离线进行。在这个例子中，我们需要实时在线解析消息。这个过程我们可以让ChatGPT在生成回复时顺便一起完成。 

- **实时干预**：实时干预是我们应该关注的事项，或者说需要设计这样的模块。一方面，有时候即便做了限制，也依然有可能被某些问法问到不太合适的答复；另一方面，不能排除部分恶意用户对机器人进行攻击。因此，最好有干预机制。在这里，我们设计了一个简单的策略：检测用户是否提问敏感问题，如果发现此类问题，直接返回预先设定好的文本，不再调用ChatGPT进行回复。 

- **更新策略**：更新策略主要是对企业知识库进行更新，这里由于我们使用的是In-Context学习能力，因此不需要调整ChatGPT，但可能需要调整Embedding接口。本例暂不涉及。 



综上所述，我们需要先对用户输入进行敏感性检查，确认没问题后开始对话，同时应存储用户消息，并在每轮对话中将用户历史消息传递给接口。 

我们先来看一下敏感性检查，这个接口比较多，国内很多厂商都有提供，我们以OpenAI就提供了一个相关的接口。这个接口本身是与对话无关的，我们以OpenAI的接口为例。

```python
import openai
import os
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
import requests
def check_risk(inp: str) -> bool:
    safe_api = "https://api.openai.com/v1/moderations"
    resp = requests.post(safe_api, json={"input": inp}, headers= {"Authorization": f"Bearer {OPENAI_API_KEY}"})
    data = resp.json()
    return data["results
