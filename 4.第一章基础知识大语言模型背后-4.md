### 1.3.3 利器强化学习RLHF流程与思想
RLHF（reinforcement learning from human feedback，从人类反馈中强化学习）听起来有点平淡无奇。确实，RLHF的思想非常朴素、简单，

但它有着不可忽视的效果。刚刚我们已经提到了，GPT - 3论文指出未来要找到新的训练方法，其中就包括从人类那里学习目标函数、强化学习微调、多模态等。时至今日，从InstructGPT到ChatGPT，再到GPT - 4，人类正一步一步地实现这些新的训练方法。这里有一点需要提醒，这些方向并不是一开始就清晰地摆在那里的，中间还有非常多的探索和阶段性成果（既有OpenAI自己的研究，也有其他从业人员的研究）。千万不要

看到结果觉得平淡无奇，这中间的艰难探索永远值得尊敬。另外，有时候即便知道了方法，要做出来，还要做出效果来，也是非常有难度的。而
且本书只能介绍少部分内容，虽然整体结构比较完整，但总体还是比较简单。总的来说，要做出来很有难度，不过我们如果只是用的话，如前所述，有手就行。

好了，言归正传，RLHF被人熟知应该主要源自OpenAI的InstructGPT论文“Training language models to follow instructions with human feedback”，更大范围的熟知就是ChatGPT的发布。因为后者没有论文发表，也没有开源，所以我们也只能“拿InstructGPT的管窥一窥ChatGPT的豹”。当然，如果按照ChatGPT官方页面上的说法，ChatGPT是InstructGPT的姐妹版，那么这个“管”可能还比较粗。如果用简单的语言来描述InstructGPT，其实就是用强化学习的算法微调一个根据人类反馈来加以改进的语言模型，重要的是还调出了效果——规模为130亿的InstructGPT堪比规模为1750亿的GPT - 3。

现在我们来看看具体是如何做的，RLHF在其中又起了什么作用，以及如何起作用。InstructGPT的整个流程分为三个步骤，如图1 - 11所示。

![image](https://github.com/user-attachments/assets/20b9b470-558d-44a0-ac32-a5913c538335)


**图1 - 11 InstructGPT流程图（摘自InstructGPT论文“Training language models to follow instructions with human feedback”）**

- **步骤一**：SFT（supervised fine - tuning，有监督微调）。顾名思义，SFT是在有监督（有标注）数据上微调训练得到的。这里的有监
督数据其实就是输入提示词，输出相应的回复，只不过这里的回复是人工编写的。这个工作要求比一般标注要高，其实算是一种创作。

- **步骤二**：RM（reward model，奖励模型）。具体来说，将一个提示词丢给前一步的SFT，输出若干（4 - 9个）回复，由标注人员对这些回复进行排序。然后从4 - 9个回复中每次取两个，因为是有序的，所以可以用来训练RM，让模型学习到好坏评价。这一步非常关键，它就是所谓的人类反馈（human feedback），用于引导下一步模型的更新方向。

- **步骤三**：RL（reinforcement learning，强化学习），使用PPO进行训练。

PPO（proximal policy optimization，近端策略优化）是一种强化学习优化方法，它背后的主要思想是避免每次太大的更新，提高训练的稳定性。具体过程如下：首先初始化一个语言模型，然后丢给它一个提示词，生成一个回复，用上一步的RM给这个回复打分，将这个打分回传给模型更新参数。这里的语言模型在强化学习视角下就是一个策略。这一步有个很重要的动作，就是在更新模型时考虑模型每一个Token的输出和SFT输出之间的差异性，要让它们尽量相似。这是为了缓解强化学习可能的过度优化。

就这样？对，就这样，RLHF都表现在上面了，效果大家都知道了。虽然ChatGPT没有相关论文发表，但我们基本相信它也是基于类似的思路实现的。当然，这里面细节非常多，即便知道了这个思路，也不一定能复现出来。这在深度学习时代很正常，里面的各种小设计、小细节实在太多了。当它们堆积到一定量时，造成的差别是很难一下子弥补的，如果别人不告诉你，那你就只能自己慢慢做实验去逐步验证了。

下面我们强行解释一下RLHF是如何起作用的，以及为什么它现在能成为一个基本的范式。其实，对于将强化学习用在NLP领域一直以来都有研究，正好笔者也由于一些原因一直在关注文本生成，以及强化学习在文本生成方面的研究。这里可能有两个难点：一是训练的稳定性；二是奖励函数的设计。前者有PPO与SFT的差异衡量，得到不小的改进；而对于后者，如果要从客观角度考虑设计一个规则，就不那么容易了。笔者也曾设想过很多类似的方法，比如加入一些语法规则限制，甚至加入类似最省力法则这样的规则。

**最省力法则**：是由齐夫在《Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology》一书中提出的。简单来说，就是语言具有惰性，它会朝着使用较少的词语表达尽可能多的语义这个方向演化。

InstructGPT使用人类反馈直接作为“规则”，把这种“规则”给隐式化，当作黑盒。我们只管结果好坏，至于中间有什么规则，有多少种规则，怎么起作用，统统不关心。这是和深度学习类似的思路，相比而言，我们之前的想法可能有些过于想当然了，毕竟语言学本身也有不少争议，认识并没有得到统一，比如语言能力是不是人与生俱来的能力？InstructGPT的做法则更加简单、直接，而且有效。

剩下要解决的就是怎么衡量“好坏”，毕竟最终是要有个结果的，既然要结果，就要有标准。读者不妨思考一下，如果换作你，你会如何设计一些指标来衡量两段输出内容的好坏。这一步看似容易，其实特别难，因为指标的设计会影响到模型的学习方向，最终会影响到效果。因为这个输出的好坏衡量标准太多了，虽然看起来是对给出的几个结果进行排序（上文的步骤二），但其实这个过程中间隐藏了大量人类的认知，模型训练过程其实就是和步骤二这个衡量过程对齐的过程；所以，如果步骤二指标没设计好，步骤三就会白费力气。尤其是对于InstructGPT这样要完成大量不同任务的设计，衡量就更加不容易。以一个文本摘要任务为例，我们可能最关注的是能否准确概括原文信息，而一个生成任务可能更关注流畅性和前后逻辑一致性。InstructGPT里面有10种任务，分别针对每种任务设计指标，不仅麻烦，而且效果还不一定好，因为这些指标并不一定都是一个方向。还有就是，万一又有了一个新任务，难道要再去设计一套指标，全部重新训练一遍模型吗？

让我们来看看InstructGPT是怎么设计衡量指标的，笔者觉得这是InstructGPT论文最宝贵的地方，也是最值得我们思考和实践的地方。感兴趣的读者可以进一步阅读笔者之前写的一篇专门介绍ChatGPT标注的文章《ChatGPT标注指南：任务、数据与规范》。首先，InstructGPT用了三大通用指标——有帮助、真实性和无害性，有点类似于阿西莫夫的机器人三定律。也就是说，不管是什么任务，都得朝着这三个方向靠拢。这个想法值得称赞。现在我们看到这个结果了，自然感觉好像没什么，但如果事先不知道要去设计出来，大部分人可能还是很容易陷入被任务影响的境地。其实，OpenAI团队在“In - Context”学习能力上的坚持也是一样的。当别人告诉你那个结果时，你可能觉得好像没有什么，甚至很多研究机构、研究人员都有过这种想法。但在有效果之前，笃信一条罕有人走的路，且一直坚定不移地走下去，这是很不容易的。

有了刚刚的三大通用指标，接下来就是细化，使其具有可操作性。比如，对于通用指标“有帮助”，InstructGPT给了一些属于“有帮助”行为的示例，如下所示。

- 用清晰的语言写作。

- 回答他们想问的问题，即使问错了，也要回答。

- 对国际性敏感（比如“football”不应该指美式足球，“总统”不一定指美国总统）。

- 如果指令（instruction）太让人困惑，要求澄清并解释指令为什么让人困惑。

- 不给出过长或冗长的答案，或重复问题中的信息。

- 不在给定的内容之外假设无关的额外上下文，除非是关于世界的事实，或是任务的隐含部分。比如，如果要求“礼貌地回复这封电子邮件：{邮件内容}”，则输出不应该假设“我这次不能来，但下周末有空”。但如果要求“给苏格拉底写一封电子邮件”，则可以放心地使用上面的假设。

笔者相信实际上这个列表可能很长，有很多例子会在实际标注过程中被依次添加进去，直到能覆盖绝大多数情况为止。即对于大部分要标注的数据，根据提供的细则很容易就判断出来是否“有帮助”。现在不妨停下来思考一下，如果一开始就奔着这些细则设计奖励规则——只是想想就觉得不太现实。其他两个通用指标也有一些示例，这里不赘述，感兴趣的读者可以阅读上面提到的笔者之前写的那篇文章，以及这篇文章最后所列的参考资料（因为有些文档资料在这篇文章中并没有提及）。

有了细则还没完，接下来要解决的是指标之间的冲突权衡问题。因为这是一个比较任务（比较哪个输出好），当涉及多个指标时，一定会出现A指标的一个结果好于另一个结果，但B指标可能相反的情况。指标越多，情况越复杂（好在只有三个指标）。对此，InstructGPT也给出了指导原则。

- 对于大部分任务，无害性和真实性比有帮助更加重要。

- 然而，如果一个输出比另一个输出更有帮助，或者该输出只是稍微不那么真实或无害，又或者该任务似乎不属于“高风险领域”（有贷款申请、医疗、法律咨询等），则更有帮助的输出得分更高。 

- 当选择同样有帮助但以不同方式不真实或有害时，问自己哪个输出更有可能对用户（现实世界中受任务影响最大的人）造成伤害。这个输出应该排名较低。如果在任务中不清楚这一点，则将这些输出标记为并列。

对于边界样例的总体指导原则是，你更愿意从试图帮助你完成此任务的客户助理那里收到哪种输出？这是一种设身处地的原则，把自己假想为任务提出者，然后问自己期望得到哪种输出。

看看这些，你是不是也觉得这一步没那么容易了，它们虽然看起来没那么“技术性”，想要很好地完成却需要优秀的设计能力、宏观把控能力和细节感知能力。笔者更加相信这些细则是自底向上逐步构建起来的，而不是一开始就设想好的。它一定是在实践中不断产生疑惑，然后经过仔细分析权衡，逐步加入一条条规则，最终逐步构建起来的一整套系统方案。笔者觉得这套系统方案可能是比数据还要珍贵的资产，它所产生的壁垒是用时间不断实践堆积出来的。

InstructGPT或ChatGPT相比GPT - 3有更强的零样本能力，少样本很多时候已经用不着，但提示词还是需要的，由此催生了一个新的行当——提示工程。不过，据OpenAI的CEO在一次采访中所言，再过几年提示工程也不需要了（可能在生成图片时还需要一些），用户要做的就是直接通过自然语言和人工智能交互。我们无法判断他说的会不会真的实现，但有一点可以肯定，人工智能的门槛必定会进一步降低，再过几年，可能一名初中生都能通过已有的服务创造出不错的人工智能应用。

### 1.4 本章小结
我们正在经历并进入一个新的时代，大语言模型作为一个外部“最强大脑”，未来一定会非常容易地被每个人获取，至于用来做什么，取决于你的想象力。无论对于哪个行业，相信这都是一个令人振奋的信号，笔者就经常激动到夜不能寐。面对这种大变革，我们能做什么呢？笔者不知道，未来有太多可能，但我们相信最好的办法就是拥抱它。让我们拥抱大语言模型，一起创造时代，创造未来。我们相信世界必将因此而变得更加美好。 
